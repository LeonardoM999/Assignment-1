{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
        "\n",
        "**Keywords**: POS tagging, Sequence labelling, RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt7RHQOnW4ly"
      },
      "source": [
        "\n",
        "# Contact\n",
        "\n",
        "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "* Eleonora Mancini -> e.mancini@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6w-qnC9W4lz"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "You are tasked to address the task of POS tagging.\n",
        "\n",
        "<center>\n",
        "        <img src=\"https://github.com/LeonardoM999/NLP/blob/main/Assignment%201/images/pos_tagging.png?raw=1\" alt=\"POS tagging\" />\n",
        "</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8ZoToQ2zTeK",
        "outputId": "b9f16fd9-d35d-432d-d2a1-3f23268bac17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.14.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "SenUuFc8KGTU"
      },
      "outputs": [],
      "source": [
        "# Necessary Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import urllib\n",
        "import sys\n",
        "import zipfile\n",
        "\n",
        "import tqdm\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import text\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "import os\n",
        "from typing import List, Callable, Dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up3ICMFMW4lz"
      },
      "source": [
        "# [Task 1 - 0.5 points] Corpus\n",
        "\n",
        "You are going to work with the [Penn TreeBank corpus](https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip).\n",
        "\n",
        "**Ignore** the numeric value in the third column, use **only** the words/symbols and their POS label.\n",
        "\n",
        "### Example\n",
        "\n",
        "```Pierre\tNNP\t2\n",
        "Vinken\tNNP\t8\n",
        ",\t,\t2\n",
        "61\tCD\t5\n",
        "years\tNNS\t6\n",
        "old\tJJ\t2\n",
        ",\t,\t2\n",
        "will\tMD\t0\n",
        "join\tVB\t8\n",
        "the\tDT\t11\n",
        "board\tNN\t9\n",
        "as\tIN\t9\n",
        "a\tDT\t15\n",
        "nonexecutive\tJJ\t15\n",
        "director\tNN\t12\n",
        "Nov.\tNNP\t9\n",
        "29\tCD\t16\n",
        ".\t.\t8\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eRPACCrW4lz"
      },
      "source": [
        "### Splits\n",
        "\n",
        "The corpus contains 200 documents.\n",
        "\n",
        "   * **Train**: Documents 1-100\n",
        "   * **Validation**: Documents 101-150\n",
        "   * **Test**: Documents 151-199"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hTffArEW4lz"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* **Download** the corpus.\n",
        "* **Encode** the corpus into a pandas.DataFrame object.\n",
        "* **Split** it in training, validation, and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZLU7yxgKiji"
      },
      "source": [
        "###Download the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "U4iN-j0aKgpR"
      },
      "outputs": [],
      "source": [
        "def download_url(download_path: Path, url: str):\n",
        "        urllib.request.urlretrieve(url, filename=download_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jA5_Oax-Kq8d",
        "outputId": "dbad0e14-700e-404b-aecc-a423f7a7f342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset... Download complete!\n",
            "Extracting dataset... (it may take a while...) Extraction completed!\n"
          ]
        }
      ],
      "source": [
        "dataset_url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
        "dataset_name = \"dependency_treebank\"\n",
        "\n",
        "#print(f\"Current work directory: {Path.cwd()}\")\n",
        "dataset_folder = Path.cwd().joinpath(\"Datasets\")\n",
        "if not dataset_folder.exists():\n",
        "    dataset_folder.mkdir(parents=True)\n",
        "\n",
        "dataset_zip_path = dataset_folder.joinpath(\"dependency_treebank.zip\")\n",
        "if not dataset_zip_path.exists():\n",
        "  print(\"Downloading dataset... \", end=\"\")\n",
        "  download_url(url=dataset_url, download_path=dataset_zip_path)\n",
        "  print(\"Download complete!\")\n",
        "else:\n",
        "  print(\"Dataset already downloaded!\")\n",
        "dataset_path = dataset_folder.joinpath(dataset_name)\n",
        "\n",
        "if not dataset_path.exists():\n",
        "  print(\"Extracting dataset... (it may take a while...) \", end=\"\")\n",
        "  shutil.unpack_archive(dataset_zip_path, dataset_folder)\n",
        "  print(\"Extraction completed!\")\n",
        "else:\n",
        "  print(\"Dataset already extracted!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-YRteY0KvQV"
      },
      "source": [
        "###Encode the corpus into a pandas DataFrame object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Z2pAOLcRKzwr"
      },
      "outputs": [],
      "source": [
        "folder = dataset_folder.joinpath(dataset_name)\n",
        "\n",
        "\n",
        "dataframe_rows = []\n",
        "for file_path in sorted(folder.glob('*.dp')):\n",
        "  with file_path.open(mode='r', encoding='utf-8') as text_file:\n",
        "    # Reading the text\n",
        "    text = text_file.read()\n",
        "    # Split sentences (\\n\\n is used for most NLP datasets to split sentences)\n",
        "    sentences = text.split(\"\\n\\n\")\n",
        "\n",
        "    # Observing each sentence\n",
        "    for s in sentences:\n",
        "      sentence = []\n",
        "      tags =[]\n",
        "      #sentence = [pierre,vinken,,aksjdajs, ]. tags = [NNP,aab,asd....]\n",
        "      # Taking every line\n",
        "      for line in s.split(\"\\n\"):\n",
        "        columns = line.split(\"\\t\")\n",
        "        # If every line have word, tag, value\n",
        "        if len(columns) > 2:\n",
        "          # Put words and tags into lists\n",
        "          sentence.append(columns[0])\n",
        "          tags.append(columns[1])\n",
        "\n",
        "      # Get the File_ID\n",
        "      file_id = int(file_path.stem.split(\"_\")[1])\n",
        "      dataframe_row = {\n",
        "               \"file_id\": file_id,\n",
        "               \"sentence\": sentence,\n",
        "               \"tag\": tags\n",
        "           }\n",
        "      dataframe_rows.append(dataframe_row)\n",
        "# Create the dataframe\n",
        "df = pd.DataFrame(dataframe_rows)\n",
        "\n",
        "FILE_ID, WORD, TAG = df.columns.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8Yemn81GK49z",
        "outputId": "3e445a89-1c6a-45f9-d66a-8ec3133b1537"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   file_id                                           sentence  \\\n",
              "0        1  [Pierre, Vinken, ,, 61, years, old, ,, will, j...   \n",
              "1        1  [Mr., Vinken, is, chairman, of, Elsevier, N.V....   \n",
              "2        2  [Rudolph, Agnew, ,, 55, years, old, and, forme...   \n",
              "3        3  [A, form, of, asbestos, once, used, to, make, ...   \n",
              "4        3  [The, asbestos, fiber, ,, crocidolite, ,, is, ...   \n",
              "\n",
              "                                                 tag  \n",
              "0  [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...  \n",
              "1  [NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...  \n",
              "2  [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...  \n",
              "3  [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...  \n",
              "4  [DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-af8d2ef6-4f83-42f1-a114-b75325cd83de\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_id</th>\n",
              "      <th>sentence</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>[Pierre, Vinken, ,, 61, years, old, ,, will, j...</td>\n",
              "      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[Mr., Vinken, is, chairman, of, Elsevier, N.V....</td>\n",
              "      <td>[NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[Rudolph, Agnew, ,, 55, years, old, and, forme...</td>\n",
              "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[A, form, of, asbestos, once, used, to, make, ...</td>\n",
              "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>[The, asbestos, fiber, ,, crocidolite, ,, is, ...</td>\n",
              "      <td>[DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-af8d2ef6-4f83-42f1-a114-b75325cd83de')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-af8d2ef6-4f83-42f1-a114-b75325cd83de button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-af8d2ef6-4f83-42f1-a114-b75325cd83de');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1238f7f0-d61f-4dd2-845d-6229369157d0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1238f7f0-d61f-4dd2-845d-6229369157d0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1238f7f0-d61f-4dd2-845d-6229369157d0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e49wfvEiu94R"
      },
      "source": [
        "### Splitting Data Train-Test-Validation\n",
        "Before splitting, lower case convertion is done as a mini preprocessing step. Main preprocessing steps will be done in further."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5AvGJaG8mwb"
      },
      "source": [
        "#### Lower Case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "tKyzpjXExkT4"
      },
      "outputs": [],
      "source": [
        "### Make a list lowercase\n",
        "def lowercase_list(input_list):\n",
        "    return [item.lower() for item in input_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "TL5t8UZp8hbQ"
      },
      "outputs": [],
      "source": [
        "df['sentence'] = df['sentence'].apply(lowercase_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvHczRup83iL"
      },
      "source": [
        "#### Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "d5DZrvZp896q"
      },
      "outputs": [],
      "source": [
        "### file indices for train/validation/test no randomization\n",
        "\n",
        "\n",
        "train_ids = np.arange(1, 101)\n",
        "val_ids = np.arange(101,151)\n",
        "test_ids = np.arange(151,200)\n",
        "\n",
        "df_train = df[df[FILE_ID].isin(train_ids)]\n",
        "df_val = df[df[FILE_ID].isin(val_ids)]\n",
        "df_test = df[df[FILE_ID].isin(test_ids)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qf53B_dW4lz"
      },
      "source": [
        "# [Task 2 - 0.5 points] Text encoding\n",
        "\n",
        "To train a neural POS tagger, you first need to encode text into numerical format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kiq6LDlKW4lz"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Embed words using **GloVe embeddings**.\n",
        "* You are **free** to pick any embedding dimension.\n",
        "* [Optional] You are free to experiment with text pre-processing: **make sure you do not delete any token!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVS7tWLILF0i"
      },
      "source": [
        "### Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjTucMegLyaD"
      },
      "source": [
        "#### Reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "73G-eZgGLTQE"
      },
      "outputs": [],
      "source": [
        "def set_reproducibility(seed):\n",
        "    random.seed(seed)               # Seed for the Python built-in random module\n",
        "    np.random.seed(seed)            # Seed for NumPy\n",
        "    tf.random.set_seed(seed)        # Seed for TensorFlow\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'  # Set an environment variable for deterministic TensorFlow operations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PLVZehwupD8"
      },
      "source": [
        "#### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "VadO_0Ymunju"
      },
      "outputs": [],
      "source": [
        "max_sequence_length=int(np.quantile([len(seq) for seq in df_train['sentence']], 0.99))\n",
        "hparams = {\n",
        "    \"batch_size\": 128,\n",
        "    \"embedding_dim\": 100,\n",
        "    \"embedding_trainable\": False,\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"max_sequence_length\": max_sequence_length,\n",
        "    \"vocab_size\" : 7405,\n",
        "    \"tag_size\" : 46\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TS56b_uruIU"
      },
      "source": [
        "#### Vocabulary Creation & Tokenization\n",
        "In order to embed the words, we need a Vocabulary and Tokenized Tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "wZKfEjNosQ4a"
      },
      "outputs": [],
      "source": [
        "### Use Keras Tokenizer to create Vocabulary\n",
        "\n",
        "tokenizer = Tokenizer(oov_token = 'OOV')\n",
        "tokenizer.fit_on_texts(df_train['sentence'])\n",
        "\n",
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(df_train['tag'])\n",
        "\n",
        "# Turns text into into padded sequences.\n",
        "def prep_text(texts, tokenizer, max_sequence_length):\n",
        "    text_sequences = tokenizer.texts_to_sequences(texts)\n",
        "    return sequence.pad_sequences(text_sequences, maxlen=max_sequence_length,padding='post')\n",
        "\n",
        "text_train = prep_text(df_train[\"sentence\"], tokenizer, hparams[\"max_sequence_length\"])\n",
        "text_test = prep_text(df_test[\"sentence\"], tokenizer, hparams[\"max_sequence_length\"])\n",
        "text_val = prep_text(df_val[\"sentence\"], tokenizer, hparams[\"max_sequence_length\"])\n",
        "\n",
        "tag_train = prep_text(df_train['tag'], tag_tokenizer, hparams[\"max_sequence_length\"])\n",
        "tag_test = prep_text(df_test['tag'], tag_tokenizer, hparams[\"max_sequence_length\"])\n",
        "tag_val = prep_text(df_val['tag'], tag_tokenizer, hparams[\"max_sequence_length\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wouRvNdIclww",
        "outputId": "67852e4d-2cb7-4d1f-8a52-ca61bedb83a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1963, 56)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "text_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "laIPfByCyf1y"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical\n",
        "num_classes = len(tag_tokenizer.word_index) + 1\n",
        "y_train = to_categorical(tag_train, num_classes)\n",
        "y_test = to_categorical(tag_test, num_classes)\n",
        "y_val = to_categorical(tag_val, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAMTQrCY9J0V",
        "outputId": "f90bad66-592b-4a4c-f0ca-d19604e745fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All classes are: ['nn', 'nnp', 'in', 'dt', 'nns', 'jj', ',', '.', 'vbd', 'rb', 'cd', 'vb', 'cc', 'vbz', 'vbn', 'to', 'prp', 'vbg', 'vbp', 'md', 'prp$', '``', 'pos', \"''\", '$', ':', 'wdt', 'jjr', 'wp', 'rp', 'nnps', 'jjs', 'wrb', 'rbr', '-rrb-', '-lrb-', 'ex', 'rbs', 'ls', 'pdt', 'wp$', 'fw', 'uh', 'sym', '#']\n",
            "Their translation in token is: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]\n",
            "\n",
            "Not Allow token (Punctuation) are: [',', '.', ':', '``', \"''\", '$', '#', 'sym', '-rrb-', '-lrb-']\n",
            "Their translation in token is: [7, 8, 26, 22, 24, 25, 45, 44, 35, 36]\n",
            "\n",
            "Classes without punctuation: ['nn', 'nnp', 'in', 'dt', 'nns', 'jj', 'vbd', 'rb', 'cd', 'vb', 'cc', 'vbz', 'vbn', 'to', 'prp', 'vbg', 'vbp', 'md', 'prp$', 'pos', 'wdt', 'jjr', 'wp', 'rp', 'nnps', 'jjs', 'wrb', 'rbr', 'ex', 'rbs', 'ls', 'pdt', 'wp$', 'fw', 'uh']\n",
            "Their translation in token is: [1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 27, 28, 29, 30, 31, 32, 33, 34, 37, 38, 39, 40, 41, 42, 43]\n"
          ]
        }
      ],
      "source": [
        "all_classes = list(tag_tokenizer.word_index.keys())\n",
        "all_tokens = list(tag_tokenizer.word_index.values())\n",
        "punct_classes = [\",\", \".\", \":\", \"``\", \"''\", \"$\", \"#\", \"sym\", \"-rrb-\", \"-lrb-\"]\n",
        "punct_tokens = [tag_tokenizer.word_index[p] for p in punct_classes]\n",
        "allowed_classes = [word for word in tag_tokenizer.index_word.values() if word not in punct_classes]\n",
        "allowed_tokens = [token for token in all_tokens if token not in punct_tokens]\n",
        "\n",
        "print(f\"All classes are: {all_classes}\\n\" +\n",
        "      f\"Their translation in token is: {all_tokens}\\n\\n\" +\n",
        "      f\"Not Allow token (Punctuation) are: {punct_classes}\\n\" +\n",
        "      f\"Their translation in token is: {punct_tokens}\\n\\n\" +\n",
        "      f\"Classes without punctuation: {allowed_classes}\\n\" +\n",
        "      f\"Their translation in token is: {allowed_tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1yfR6pm1It5",
        "outputId": "09932884-c37c-48c1-d5f7-97dc0912e825"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1963, 56, 46)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3fD-c1TGC05"
      },
      "source": [
        "### Glove Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQq47Yl5GlF-"
      },
      "source": [
        "#### Downloading Pre-Trained Glove Embeddings\n",
        "This may take a few minutes to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "PP4cd3efGN1b"
      },
      "outputs": [],
      "source": [
        "zip_file_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "zip_file = urllib.request.urlopen(zip_file_url)\n",
        "archive = zipfile.ZipFile(io.BytesIO(zip_file.read()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j53UlYj8acCl"
      },
      "source": [
        "#### Creating Embedding Matrix\n",
        "We use the downloaded GloVe embeddings to create an embedding matrix, where the rows contain the word embeddings for the tokens in the Tokenizer's vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "0ew3ewjT7kwt"
      },
      "outputs": [],
      "source": [
        "embeddings_index = {}\n",
        "glove_file = \"glove.6B.100d.txt\"\n",
        "\n",
        "with archive.open(glove_file) as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0].decode(\"utf-8\")\n",
        "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, hparams[\"embedding_dim\"]))\n",
        "num_words_in_embedding = 0\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        num_words_in_embedding += 1\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXtFPiCfAQxy",
        "outputId": "55e0dca1-b1d0-4706-f5b3-0758b2065d52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: the \n",
            "Vector: [-0.10767     0.11053     0.59811997 -0.54360998  0.67395997  0.10663\n",
            "  0.038867    0.35481     0.06351    -0.094189    0.15786    -0.81664997\n",
            "  0.14172     0.21939     0.58504999 -0.52157998  0.22782999 -0.16642\n",
            " -0.68228     0.35870001  0.42568001  0.19021     0.91962999  0.57555002\n",
            "  0.46184999  0.42363    -0.095399   -0.42749    -0.16566999 -0.056842\n",
            " -0.29595     0.26036999 -0.26605999 -0.070404   -0.27662     0.15820999\n",
            "  0.69825     0.43081     0.27952    -0.45436999 -0.33801001 -0.58183998\n",
            "  0.22363999 -0.57779998 -0.26862001 -0.20424999  0.56393999 -0.58524001\n",
            " -0.14365    -0.64218003  0.0054697  -0.35247999  0.16162001  1.1796\n",
            " -0.47674    -2.75530005 -0.1321     -0.047729    1.06550002  1.10339999\n",
            " -0.2208      0.18669     0.13177     0.15117     0.71310002 -0.35214999\n",
            "  0.91347998  0.61782998  0.70991999  0.23954999 -0.14571001 -0.37858999\n",
            " -0.045959   -0.47367999  0.2385      0.20536    -0.18996     0.32506999\n",
            " -1.11119998 -0.36341     0.98679    -0.084776   -0.54008001  0.11726\n",
            " -1.0194     -0.24424     0.12771     0.013884    0.080374   -0.35414001\n",
            "  0.34951001 -0.72259998  0.37549001  0.44409999 -0.99058998  0.61214\n",
            " -0.35111001 -0.83155     0.45293     0.082577  ]\n"
          ]
        }
      ],
      "source": [
        "### Inspect tokens' embedding vectors\n",
        "idx_token = 2\n",
        "print(f'Token: {list(tokenizer.word_index.keys())[idx_token]} \\nVector: {embedding_matrix[idx_token]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtIIKF42W4lz"
      },
      "source": [
        "# [Task 3 - 1.0 points] Model definition\n",
        "\n",
        "You are now tasked to define your neural POS tagger."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8ixaFHiW4l0"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* **Baseline**: implement a Bidirectional LSTM with a Dense layer on top.\n",
        "* You are **free** to experiment with hyper-parameters to define the baseline model.\n",
        "\n",
        "* **Model 1**: add an additional LSTM layer to the Baseline model.\n",
        "* **Model 2**: add an additional Dense layer to the Baseline model.\n",
        "\n",
        "* **Do not mix Model 1 and Model 2**. Each model has its own instructions.\n",
        "\n",
        "**Note**: if a document contains many tokens, you are **free** to split them into chunks or sentences to define your mini-batches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYJmA3YHtY_Y"
      },
      "source": [
        "### Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "B_ZlA-kqtk3G"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "KmCTiV9BBugg"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense\n",
        "import tensorflow as tf\n",
        "\n",
        "class CreateModel(tf.keras.Model):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        vocab_size = config['vocab_size']\n",
        "        embedding_dim = config['embedding_dim']\n",
        "        max_sequence_length = config['max_sequence_length']\n",
        "        embedding_matrix = config['embedding_matrix']\n",
        "        tag_size = config['tag_size']\n",
        "        lstm_units = config['lstm_units']\n",
        "        Additional_LSTM = config['Additional_LSTM']\n",
        "        Additional_Dense = config['Additional_Dense']\n",
        "        add_lstm_units = config['add_lstm_units']\n",
        "        add_dense_units = config['add_dense_units']\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding_layer = Embedding(\n",
        "            vocab_size + 1,\n",
        "            embedding_dim,\n",
        "            input_length=max_sequence_length,\n",
        "            weights=[embedding_matrix],\n",
        "            trainable=False\n",
        "        )\n",
        "\n",
        "        # Bidirectional LSTM layer\n",
        "        self.bi_lstm = Bidirectional(LSTM(lstm_units, return_sequences=True))\n",
        "        # Additional LSTM\n",
        "        self.additional_lstm = Bidirectional(LSTM(add_lstm_units, return_sequences=True)) if Additional_LSTM else None\n",
        "        # Additional Dense\n",
        "        self.additional_dense = Dense(add_dense_units, activation='softmax') if Additional_Dense else None\n",
        "\n",
        "        # Dense output layer\n",
        "        self.dense_output = Dense(tag_size, activation='softmax')\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Define the forward pass\n",
        "        x = self.embedding_layer(inputs)\n",
        "        x = self.bi_lstm(x)\n",
        "\n",
        "        # Add the additional LSTM layer if specified\n",
        "        if self.additional_lstm:\n",
        "            x = self.additional_lstm(x)\n",
        "\n",
        "        # Add the additional Dense layer if specified\n",
        "        if self.additional_dense:\n",
        "            x = self.additional_dense(x)\n",
        "\n",
        "        outputs = self.dense_output(x)\n",
        "        return outputs\n",
        "\n",
        "    def build(self, shape):\n",
        "        x = tf.keras.layers.Input(shape=(shape,))\n",
        "        return tf.keras.Model(inputs=x, outputs=self.call(x))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP4qZYKTDKC8",
        "outputId": "573fadb0-843a-4563-cb1d-132d28e89f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 56)]              0         \n",
            "                                                                 \n",
            " embedding_3 (Embedding)     (None, 56, 100)           740600    \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, 56, 128)           84480     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 56, 46)            5934      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 831014 (3.17 MB)\n",
            "Trainable params: 90414 (353.18 KB)\n",
            "Non-trainable params: 740600 (2.83 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "config_dict = {\n",
        "    'vocab_size': 7405,\n",
        "    'embedding_dim': 100,\n",
        "    'max_sequence_length': max_sequence_length,\n",
        "    'embedding_matrix': embedding_matrix,\n",
        "    'tag_size': 46,\n",
        "    'lstm_units': 64,\n",
        "    'Additional_LSTM': False,\n",
        "    'Additional_Dense': False,\n",
        "    'add_lstm_units': None,\n",
        "    'add_dense_units': None\n",
        "}\n",
        "# Create an instance of the custom model\n",
        "custom_model = CreateModel(config_dict).build(config_dict[\"max_sequence_length\"])\n",
        "\n",
        "# Compile the model\n",
        "custom_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "\n",
        "custom_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSyXOO7eVwRE"
      },
      "source": [
        "### TestCode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmvhbCTntbuR",
        "outputId": "9a6bb607-d026-4e42-9210-f77a3f10d415"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 56)]              0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 56, 100)           740600    \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 56, 512)           731136    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dense (Dense)               (None, 56, 46)            23598     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1495334 (5.70 MB)\n",
            "Trainable params: 754734 (2.88 MB)\n",
            "Non-trainable params: 740600 (2.83 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# ### Baseline: implement a Bidirectional LSTM with a Dense layer on top.\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense\n",
        "\n",
        "# # Define input layer\n",
        "# inputs = Input(shape=(hparams[\"max_sequence_length\"],))\n",
        "\n",
        "# # Add embedding layer\n",
        "# embedding_layer = Embedding(hparams[\"vocab_size\"]+1, hparams[\"embedding_dim\"], input_length=hparams[\"max_sequence_length\"], weights=[embedding_matrix], trainable=False)(inputs)\n",
        "\n",
        "# # Add bidirectional LSTM layer\n",
        "# bi_lstm = Bidirectional(LSTM(256, return_sequences=True))(embedding_layer)\n",
        "\n",
        "# # Add dense output layer\n",
        "# outputs = Dense(hparams[\"tag_size\"], activation='softmax')(bi_lstm)\n",
        "\n",
        "# # Create the model\n",
        "# model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed9Q8yZ5xsXG"
      },
      "outputs": [],
      "source": [
        "history = model.fit(text_train, y_train, batch_size=64, epochs=50, validation_data=(text_val,y_val) , verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "rAHQltBV6-V6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQQUOcIe77vf",
        "outputId": "32e7c138-fecb-4a85-df09-6dd13d06f52f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21/21 [==============================] - 1s 8ms/step\n",
            "F1 score: 0.7255384045552715\n"
          ]
        }
      ],
      "source": [
        "y_pred = model.predict([text_test]).argmax(-1).flatten()\n",
        "y_test_flatten = y_test.argmax(-1).flatten()\n",
        "print(\"F1 score: {}\".format(f1_score(y_test_flatten, y_pred, labels=allowed_tokens, average='macro', zero_division=0)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbYrkc36W4l0"
      },
      "source": [
        "# [Task 4 - 1.0 points] Metrics\n",
        "\n",
        "Before training the models, you are tasked to define the evaluation metrics for comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjyhLkpzW4l0"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Evaluate your models using macro F1-score, compute over **all** tokens.\n",
        "* **Concatenate** all tokens in a data split to compute the F1-score. (**Hint**: accumulate FP, TP, FN, TN iteratively)\n",
        "* **Do not consider punctuation and symbol classes** $\\rightarrow$ [What is punctuation?](https://en.wikipedia.org/wiki/English_punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxvARG0XW4l0"
      },
      "source": [
        "**Note**: What about OOV tokens?\n",
        "   * All the tokens in the **training** set that are not in GloVe **must** be added to the vocabulary.\n",
        "   * For the remaining tokens (i.e., OOV in the validation and test sets), you have to assign them a **special token** (e.g., [UNK]) and a **static** embedding.\n",
        "   * You are **free** to define the static embedding using any strategy (e.g., random, neighbourhood, etc...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0tOgd29W4l0"
      },
      "source": [
        "### More about OOV\n",
        "\n",
        "For a given token:\n",
        "\n",
        "* **If in train set**: add to vocabulary and assign an embedding (use GloVe if token in GloVe, custom embedding otherwise).\n",
        "* **If in val/test set**: assign special token if not in vocabulary and assign custom embedding.\n",
        "\n",
        "Your vocabulary **should**:\n",
        "\n",
        "* Contain all tokens in train set; or\n",
        "* Union of tokens in train set and in GloVe $\\rightarrow$ we make use of existing knowledge!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXMKNENFW4l0"
      },
      "source": [
        "### Token to embedding mapping\n",
        "\n",
        "You can follow two approaches for encoding tokens in your POS tagger."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V6-99t6W4l0"
      },
      "source": [
        "### Work directly with embeddings\n",
        "\n",
        "- Compute the embedding of each input token\n",
        "- Feed the mini-batches of shape (batch_size, # tokens, embedding_dim) to your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWSMq5zwW4l0"
      },
      "source": [
        "### Work with Embedding layer\n",
        "\n",
        "- Encode input tokens to token ids\n",
        "- Define a Embedding layer as the first layer of your model\n",
        "- Compute the embedding matrix of all known tokens (i.e., tokens in your vocabulary)\n",
        "- Initialize the Embedding layer with the computed embedding matrix\n",
        "- You are **free** to set the Embedding layer trainable or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocN3IS-SW4l1"
      },
      "source": [
        "### Padding\n",
        "\n",
        "Pay attention to padding tokens!\n",
        "\n",
        "Your model **should not** be penalized on those tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LweewwvkW4l1"
      },
      "source": [
        "#### How to?\n",
        "\n",
        "There are two main ways.\n",
        "\n",
        "However, their implementation depends on the neural library you are using.\n",
        "\n",
        "- Embedding layer\n",
        "- Custom loss to compute average cross-entropy on non-padding tokens only\n",
        "\n",
        "**Note**: This is a **recommendation**, but we **do not penalize** for missing workarounds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0SONHumS1A8"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP_HuIx-TBnk"
      },
      "source": [
        "#### Hyperparameter Tuning & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations_with_replacement\n",
        "\n",
        "def layer_unit_calculator(max_units):\n",
        "    base_units = [32, 64, 128, 256,512]\n",
        "    for _ in base_units:\n",
        "      if _ > max_units:\n",
        "        base_units.remove(_)\n",
        "\n",
        "\n",
        "    # Generate all possible two-element combinations\n",
        "    # Convert the resulting iterator to a list\n",
        "    layer_units = list(combinations_with_replacement(base_units, 2))\n",
        "\n",
        "    return base_units, layer_units\n",
        "\n"
      ],
      "metadata": {
        "id": "XbhkMFA6lQ6B"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Zwle2678Sz4K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "311e74f7-4616-471c-d062-2a1366e2e391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_16 (InputLayer)       [(None, 56)]              0         \n",
            "                                                                 \n",
            " embedding_16 (Embedding)    (None, 56, 100)           740600    \n",
            "                                                                 \n",
            " bidirectional_22 (Bidirect  (None, 56, 64)            34048     \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 56, 46)            2990      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 777638 (2.97 MB)\n",
            "Trainable params: 37038 (144.68 KB)\n",
            "Non-trainable params: 740600 (2.83 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "31/31 [==============================] - 10s 41ms/step - loss: 3.2922 - accuracy: 0.5718 - val_loss: 2.3395 - val_accuracy: 0.6585\n",
            "Epoch 2/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 1.6186 - accuracy: 0.6809 - val_loss: 1.2750 - val_accuracy: 0.7196\n",
            "Epoch 3/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 1.1437 - accuracy: 0.7411 - val_loss: 1.0679 - val_accuracy: 0.7501\n",
            "Epoch 4/50\n",
            "31/31 [==============================] - 0s 12ms/step - loss: 0.9637 - accuracy: 0.7694 - val_loss: 0.9173 - val_accuracy: 0.7708\n",
            "Epoch 5/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.8203 - accuracy: 0.7970 - val_loss: 0.7927 - val_accuracy: 0.8014\n",
            "Epoch 6/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.7030 - accuracy: 0.8309 - val_loss: 0.6955 - val_accuracy: 0.8308\n",
            "Epoch 7/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.6110 - accuracy: 0.8598 - val_loss: 0.6205 - val_accuracy: 0.8497\n",
            "Epoch 8/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.5395 - accuracy: 0.8786 - val_loss: 0.5620 - val_accuracy: 0.8628\n",
            "Epoch 9/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.4828 - accuracy: 0.8916 - val_loss: 0.5142 - val_accuracy: 0.8741\n",
            "Epoch 10/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.4375 - accuracy: 0.9011 - val_loss: 0.4760 - val_accuracy: 0.8830\n",
            "Epoch 11/50\n",
            "31/31 [==============================] - 0s 12ms/step - loss: 0.4007 - accuracy: 0.9096 - val_loss: 0.4451 - val_accuracy: 0.8899\n",
            "Epoch 12/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.3706 - accuracy: 0.9158 - val_loss: 0.4191 - val_accuracy: 0.8958\n",
            "Epoch 13/50\n",
            "31/31 [==============================] - 0s 12ms/step - loss: 0.3457 - accuracy: 0.9219 - val_loss: 0.3981 - val_accuracy: 0.9009\n",
            "Epoch 14/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.3245 - accuracy: 0.9268 - val_loss: 0.3804 - val_accuracy: 0.9057\n",
            "Epoch 15/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.3064 - accuracy: 0.9310 - val_loss: 0.3644 - val_accuracy: 0.9092\n",
            "Epoch 16/50\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.2905 - accuracy: 0.9337 - val_loss: 0.3507 - val_accuracy: 0.9117\n",
            "Epoch 17/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.2765 - accuracy: 0.9364 - val_loss: 0.3383 - val_accuracy: 0.9142\n",
            "Epoch 18/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.2642 - accuracy: 0.9384 - val_loss: 0.3285 - val_accuracy: 0.9156\n",
            "Epoch 19/50\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2531 - accuracy: 0.9403 - val_loss: 0.3184 - val_accuracy: 0.9175\n",
            "Epoch 20/50\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 0.2431 - accuracy: 0.9419 - val_loss: 0.3096 - val_accuracy: 0.9193\n",
            "Epoch 21/50\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2339 - accuracy: 0.9438 - val_loss: 0.3026 - val_accuracy: 0.9204\n",
            "Epoch 22/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2256 - accuracy: 0.9452 - val_loss: 0.2948 - val_accuracy: 0.9221\n",
            "Epoch 23/50\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.2179 - accuracy: 0.9465 - val_loss: 0.2885 - val_accuracy: 0.9232\n",
            "Epoch 24/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2108 - accuracy: 0.9479 - val_loss: 0.2825 - val_accuracy: 0.9243\n",
            "Epoch 25/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.2042 - accuracy: 0.9493 - val_loss: 0.2768 - val_accuracy: 0.9253\n",
            "Epoch 26/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1980 - accuracy: 0.9506 - val_loss: 0.2718 - val_accuracy: 0.9264\n",
            "Epoch 27/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1924 - accuracy: 0.9518 - val_loss: 0.2676 - val_accuracy: 0.9275\n",
            "Epoch 28/50\n",
            "31/31 [==============================] - 0s 12ms/step - loss: 0.1869 - accuracy: 0.9526 - val_loss: 0.2628 - val_accuracy: 0.9286\n",
            "Epoch 29/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1819 - accuracy: 0.9539 - val_loss: 0.2581 - val_accuracy: 0.9300\n",
            "Epoch 30/50\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.1773 - accuracy: 0.9547 - val_loss: 0.2551 - val_accuracy: 0.9297\n",
            "Epoch 31/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1726 - accuracy: 0.9557 - val_loss: 0.2521 - val_accuracy: 0.9307\n",
            "Epoch 32/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1683 - accuracy: 0.9568 - val_loss: 0.2488 - val_accuracy: 0.9315\n",
            "Epoch 33/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1642 - accuracy: 0.9576 - val_loss: 0.2442 - val_accuracy: 0.9326\n",
            "Epoch 34/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1603 - accuracy: 0.9585 - val_loss: 0.2429 - val_accuracy: 0.9326\n",
            "Epoch 35/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1566 - accuracy: 0.9593 - val_loss: 0.2391 - val_accuracy: 0.9338\n",
            "Epoch 36/50\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.1532 - accuracy: 0.9603 - val_loss: 0.2364 - val_accuracy: 0.9341\n",
            "Epoch 37/50\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.1498 - accuracy: 0.9610 - val_loss: 0.2339 - val_accuracy: 0.9347\n",
            "Epoch 38/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1465 - accuracy: 0.9617 - val_loss: 0.2323 - val_accuracy: 0.9354\n",
            "Epoch 39/50\n",
            "31/31 [==============================] - 0s 12ms/step - loss: 0.1434 - accuracy: 0.9628 - val_loss: 0.2288 - val_accuracy: 0.9360\n",
            "Epoch 40/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1405 - accuracy: 0.9634 - val_loss: 0.2273 - val_accuracy: 0.9362\n",
            "Epoch 41/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1378 - accuracy: 0.9642 - val_loss: 0.2251 - val_accuracy: 0.9369\n",
            "Epoch 42/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1349 - accuracy: 0.9651 - val_loss: 0.2234 - val_accuracy: 0.9377\n",
            "Epoch 43/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1323 - accuracy: 0.9659 - val_loss: 0.2220 - val_accuracy: 0.9379\n",
            "Epoch 44/50\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.1297 - accuracy: 0.9665 - val_loss: 0.2204 - val_accuracy: 0.9384\n",
            "Epoch 45/50\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.1272 - accuracy: 0.9675 - val_loss: 0.2184 - val_accuracy: 0.9390\n",
            "Epoch 46/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1249 - accuracy: 0.9679 - val_loss: 0.2178 - val_accuracy: 0.9392\n",
            "Epoch 47/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1226 - accuracy: 0.9685 - val_loss: 0.2166 - val_accuracy: 0.9393\n",
            "Epoch 48/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1205 - accuracy: 0.9691 - val_loss: 0.2141 - val_accuracy: 0.9398\n",
            "Epoch 49/50\n",
            "31/31 [==============================] - 0s 16ms/step - loss: 0.1182 - accuracy: 0.9697 - val_loss: 0.2122 - val_accuracy: 0.9406\n",
            "Epoch 50/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.1162 - accuracy: 0.9702 - val_loss: 0.2114 - val_accuracy: 0.9407\n",
            "21/21 [==============================] - 2s 6ms/step\n",
            "F1 score: 0.6569508848372635\n",
            "Model: \"model_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_17 (InputLayer)       [(None, 56)]              0         \n",
            "                                                                 \n",
            " embedding_17 (Embedding)    (None, 56, 100)           740600    \n",
            "                                                                 \n",
            " bidirectional_23 (Bidirect  (None, 56, 128)           84480     \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 56, 46)            5934      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 831014 (3.17 MB)\n",
            "Trainable params: 90414 (353.18 KB)\n",
            "Non-trainable params: 740600 (2.83 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "31/31 [==============================] - 5s 46ms/step - loss: 2.5060 - accuracy: 0.5924 - val_loss: 1.2994 - val_accuracy: 0.6820\n",
            "Epoch 2/50\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 1.1178 - accuracy: 0.7341 - val_loss: 1.0120 - val_accuracy: 0.7507\n",
            "Epoch 3/50\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.8709 - accuracy: 0.7883 - val_loss: 0.7982 - val_accuracy: 0.7998\n",
            "Epoch 4/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.6769 - accuracy: 0.8365 - val_loss: 0.6447 - val_accuracy: 0.8391\n",
            "Epoch 5/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.5405 - accuracy: 0.8736 - val_loss: 0.5423 - val_accuracy: 0.8610\n",
            "Epoch 6/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.4476 - accuracy: 0.8923 - val_loss: 0.4719 - val_accuracy: 0.8755\n",
            "Epoch 7/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.3835 - accuracy: 0.9067 - val_loss: 0.4225 - val_accuracy: 0.8872\n",
            "Epoch 8/50\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.3377 - accuracy: 0.9172 - val_loss: 0.3831 - val_accuracy: 0.8987\n",
            "Epoch 9/50\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.3034 - accuracy: 0.9249 - val_loss: 0.3550 - val_accuracy: 0.9063\n",
            "Epoch 10/50\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.2767 - accuracy: 0.9313 - val_loss: 0.3352 - val_accuracy: 0.9104\n",
            "Epoch 11/50\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.2552 - accuracy: 0.9361 - val_loss: 0.3153 - val_accuracy: 0.9156\n",
            "Epoch 12/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2377 - accuracy: 0.9400 - val_loss: 0.3015 - val_accuracy: 0.9182\n",
            "Epoch 13/50\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.2231 - accuracy: 0.9426 - val_loss: 0.2895 - val_accuracy: 0.9209\n",
            "Epoch 14/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2105 - accuracy: 0.9454 - val_loss: 0.2783 - val_accuracy: 0.9234\n",
            "Epoch 15/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.1995 - accuracy: 0.9480 - val_loss: 0.2677 - val_accuracy: 0.9253\n",
            "Epoch 16/50\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.1902 - accuracy: 0.9499 - val_loss: 0.2596 - val_accuracy: 0.9273\n",
            "Epoch 17/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1817 - accuracy: 0.9522 - val_loss: 0.2557 - val_accuracy: 0.9285\n",
            "Epoch 18/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1740 - accuracy: 0.9538 - val_loss: 0.2469 - val_accuracy: 0.9308\n",
            "Epoch 19/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1673 - accuracy: 0.9557 - val_loss: 0.2404 - val_accuracy: 0.9322\n",
            "Epoch 20/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1613 - accuracy: 0.9572 - val_loss: 0.2380 - val_accuracy: 0.9327\n",
            "Epoch 21/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1553 - accuracy: 0.9588 - val_loss: 0.2325 - val_accuracy: 0.9342\n",
            "Epoch 22/50\n",
            "31/31 [==============================] - 0s 15ms/step - loss: 0.1501 - accuracy: 0.9598 - val_loss: 0.2271 - val_accuracy: 0.9360\n",
            "Epoch 23/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1452 - accuracy: 0.9613 - val_loss: 0.2236 - val_accuracy: 0.9367\n",
            "Epoch 24/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1409 - accuracy: 0.9626 - val_loss: 0.2217 - val_accuracy: 0.9371\n",
            "Epoch 25/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1367 - accuracy: 0.9634 - val_loss: 0.2184 - val_accuracy: 0.9381\n",
            "Epoch 26/50\n",
            "31/31 [==============================] - 0s 12ms/step - loss: 0.1326 - accuracy: 0.9644 - val_loss: 0.2148 - val_accuracy: 0.9390\n",
            "Epoch 27/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1289 - accuracy: 0.9653 - val_loss: 0.2132 - val_accuracy: 0.9394\n",
            "Epoch 28/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1254 - accuracy: 0.9662 - val_loss: 0.2119 - val_accuracy: 0.9397\n",
            "Epoch 29/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1221 - accuracy: 0.9672 - val_loss: 0.2080 - val_accuracy: 0.9404\n",
            "Epoch 30/50\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.1187 - accuracy: 0.9678 - val_loss: 0.2043 - val_accuracy: 0.9414\n",
            "Epoch 31/50\n",
            "31/31 [==============================] - 0s 12ms/step - loss: 0.1156 - accuracy: 0.9690 - val_loss: 0.2049 - val_accuracy: 0.9415\n",
            "Epoch 32/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1123 - accuracy: 0.9698 - val_loss: 0.2015 - val_accuracy: 0.9422\n",
            "Epoch 33/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1103 - accuracy: 0.9702 - val_loss: 0.1995 - val_accuracy: 0.9424\n",
            "Epoch 34/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1070 - accuracy: 0.9713 - val_loss: 0.1981 - val_accuracy: 0.9435\n",
            "Epoch 35/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1042 - accuracy: 0.9718 - val_loss: 0.1989 - val_accuracy: 0.9430\n",
            "Epoch 36/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.1019 - accuracy: 0.9727 - val_loss: 0.1954 - val_accuracy: 0.9435\n",
            "Epoch 37/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.0993 - accuracy: 0.9733 - val_loss: 0.1959 - val_accuracy: 0.9441\n",
            "Epoch 38/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.0968 - accuracy: 0.9742 - val_loss: 0.1935 - val_accuracy: 0.9448\n",
            "Epoch 39/50\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.0943 - accuracy: 0.9747 - val_loss: 0.1940 - val_accuracy: 0.9445\n",
            "Epoch 40/50\n",
            "31/31 [==============================] - 0s 12ms/step - loss: 0.0921 - accuracy: 0.9756 - val_loss: 0.1917 - val_accuracy: 0.9451\n",
            "Epoch 41/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.0898 - accuracy: 0.9763 - val_loss: 0.1897 - val_accuracy: 0.9456\n",
            "Epoch 42/50\n",
            "31/31 [==============================] - 1s 23ms/step - loss: 0.0876 - accuracy: 0.9767 - val_loss: 0.1884 - val_accuracy: 0.9459\n",
            "Epoch 43/50\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.0856 - accuracy: 0.9775 - val_loss: 0.1900 - val_accuracy: 0.9456\n",
            "Epoch 44/50\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.0837 - accuracy: 0.9779 - val_loss: 0.1862 - val_accuracy: 0.9464\n",
            "Epoch 45/50\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.0816 - accuracy: 0.9786 - val_loss: 0.1859 - val_accuracy: 0.9468\n",
            "Epoch 46/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.0795 - accuracy: 0.9794 - val_loss: 0.1848 - val_accuracy: 0.9468\n",
            "Epoch 47/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.0777 - accuracy: 0.9799 - val_loss: 0.1869 - val_accuracy: 0.9461\n",
            "Epoch 48/50\n",
            "31/31 [==============================] - 0s 14ms/step - loss: 0.0759 - accuracy: 0.9804 - val_loss: 0.1834 - val_accuracy: 0.9471\n",
            "Epoch 49/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.0738 - accuracy: 0.9810 - val_loss: 0.1844 - val_accuracy: 0.9466\n",
            "Epoch 50/50\n",
            "31/31 [==============================] - 0s 13ms/step - loss: 0.0721 - accuracy: 0.9815 - val_loss: 0.1880 - val_accuracy: 0.9461\n",
            "21/21 [==============================] - 1s 6ms/step\n",
            "F1 score: 0.7104991122525602\n",
            "Model: \"model_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_18 (InputLayer)       [(None, 56)]              0         \n",
            "                                                                 \n",
            " embedding_18 (Embedding)    (None, 56, 100)           740600    \n",
            "                                                                 \n",
            " bidirectional_24 (Bidirect  (None, 56, 512)           731136    \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 56, 46)            23598     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1495334 (5.70 MB)\n",
            "Trainable params: 754734 (2.88 MB)\n",
            "Non-trainable params: 740600 (2.83 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "31/31 [==============================] - 6s 58ms/step - loss: 1.5504 - accuracy: 0.6661 - val_loss: 0.9825 - val_accuracy: 0.7584\n",
            "Epoch 2/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.7568 - accuracy: 0.8095 - val_loss: 0.6202 - val_accuracy: 0.8371\n",
            "Epoch 3/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.4677 - accuracy: 0.8845 - val_loss: 0.4388 - val_accuracy: 0.8829\n",
            "Epoch 4/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.3373 - accuracy: 0.9144 - val_loss: 0.3559 - val_accuracy: 0.9035\n",
            "Epoch 5/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2710 - accuracy: 0.9298 - val_loss: 0.3259 - val_accuracy: 0.9074\n",
            "Epoch 6/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2314 - accuracy: 0.9395 - val_loss: 0.2822 - val_accuracy: 0.9213\n",
            "Epoch 7/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.2051 - accuracy: 0.9457 - val_loss: 0.2732 - val_accuracy: 0.9211\n",
            "Epoch 8/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.1881 - accuracy: 0.9493 - val_loss: 0.2560 - val_accuracy: 0.9264\n",
            "Epoch 9/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.1724 - accuracy: 0.9533 - val_loss: 0.2445 - val_accuracy: 0.9299\n",
            "Epoch 10/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.1598 - accuracy: 0.9561 - val_loss: 0.2421 - val_accuracy: 0.9293\n",
            "Epoch 11/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.1501 - accuracy: 0.9587 - val_loss: 0.2230 - val_accuracy: 0.9358\n",
            "Epoch 12/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.1414 - accuracy: 0.9610 - val_loss: 0.2313 - val_accuracy: 0.9329\n",
            "Epoch 13/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.1335 - accuracy: 0.9636 - val_loss: 0.2119 - val_accuracy: 0.9382\n",
            "Epoch 14/50\n",
            "31/31 [==============================] - 1s 17ms/step - loss: 0.1269 - accuracy: 0.9650 - val_loss: 0.2128 - val_accuracy: 0.9376\n",
            "Epoch 15/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.1198 - accuracy: 0.9674 - val_loss: 0.2056 - val_accuracy: 0.9403\n",
            "Epoch 16/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.1137 - accuracy: 0.9689 - val_loss: 0.2026 - val_accuracy: 0.9406\n",
            "Epoch 17/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.1081 - accuracy: 0.9704 - val_loss: 0.2000 - val_accuracy: 0.9422\n",
            "Epoch 18/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.1023 - accuracy: 0.9723 - val_loss: 0.2016 - val_accuracy: 0.9404\n",
            "Epoch 19/50\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.0977 - accuracy: 0.9736 - val_loss: 0.1969 - val_accuracy: 0.9422\n",
            "Epoch 20/50\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.0924 - accuracy: 0.9754 - val_loss: 0.1949 - val_accuracy: 0.9425\n",
            "Epoch 21/50\n",
            "31/31 [==============================] - 1s 24ms/step - loss: 0.0889 - accuracy: 0.9760 - val_loss: 0.1999 - val_accuracy: 0.9415\n",
            "Epoch 22/50\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.0841 - accuracy: 0.9774 - val_loss: 0.1931 - val_accuracy: 0.9435\n",
            "Epoch 23/50\n",
            "31/31 [==============================] - 1s 20ms/step - loss: 0.0803 - accuracy: 0.9790 - val_loss: 0.1979 - val_accuracy: 0.9428\n",
            "Epoch 24/50\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.0758 - accuracy: 0.9803 - val_loss: 0.1916 - val_accuracy: 0.9429\n",
            "Epoch 25/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.0716 - accuracy: 0.9815 - val_loss: 0.1901 - val_accuracy: 0.9436\n",
            "Epoch 26/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.0673 - accuracy: 0.9833 - val_loss: 0.1917 - val_accuracy: 0.9430\n",
            "Epoch 27/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.0630 - accuracy: 0.9847 - val_loss: 0.1901 - val_accuracy: 0.9441\n",
            "Epoch 28/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.0595 - accuracy: 0.9859 - val_loss: 0.1926 - val_accuracy: 0.9432\n",
            "Epoch 29/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.0556 - accuracy: 0.9874 - val_loss: 0.1997 - val_accuracy: 0.9413\n",
            "Epoch 30/50\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.0529 - accuracy: 0.9882 - val_loss: 0.1898 - val_accuracy: 0.9446\n",
            "Epoch 31/50\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.0486 - accuracy: 0.9896 - val_loss: 0.1957 - val_accuracy: 0.9432\n",
            "Epoch 32/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.0459 - accuracy: 0.9902 - val_loss: 0.1958 - val_accuracy: 0.9427\n",
            "Epoch 33/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.0426 - accuracy: 0.9913 - val_loss: 0.1944 - val_accuracy: 0.9433\n",
            "Epoch 34/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.0407 - accuracy: 0.9916 - val_loss: 0.1988 - val_accuracy: 0.9422\n",
            "Epoch 35/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.0375 - accuracy: 0.9927 - val_loss: 0.1984 - val_accuracy: 0.9431\n",
            "Epoch 36/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.0337 - accuracy: 0.9944 - val_loss: 0.2011 - val_accuracy: 0.9423\n",
            "Epoch 37/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.0316 - accuracy: 0.9948 - val_loss: 0.1999 - val_accuracy: 0.9421\n",
            "Epoch 38/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.0291 - accuracy: 0.9954 - val_loss: 0.2134 - val_accuracy: 0.9406\n",
            "Epoch 39/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.0268 - accuracy: 0.9963 - val_loss: 0.2039 - val_accuracy: 0.9425\n",
            "Epoch 40/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.0237 - accuracy: 0.9972 - val_loss: 0.2094 - val_accuracy: 0.9419\n",
            "Epoch 41/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.0217 - accuracy: 0.9978 - val_loss: 0.2087 - val_accuracy: 0.9421\n",
            "Epoch 42/50\n",
            "31/31 [==============================] - 1s 26ms/step - loss: 0.0202 - accuracy: 0.9981 - val_loss: 0.2127 - val_accuracy: 0.9414\n",
            "Epoch 43/50\n",
            "31/31 [==============================] - 1s 25ms/step - loss: 0.0184 - accuracy: 0.9985 - val_loss: 0.2122 - val_accuracy: 0.9417\n",
            "Epoch 44/50\n",
            "31/31 [==============================] - 1s 25ms/step - loss: 0.0169 - accuracy: 0.9986 - val_loss: 0.2174 - val_accuracy: 0.9412\n",
            "Epoch 45/50\n",
            "31/31 [==============================] - 1s 25ms/step - loss: 0.0155 - accuracy: 0.9990 - val_loss: 0.2148 - val_accuracy: 0.9415\n",
            "Epoch 46/50\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.0141 - accuracy: 0.9992 - val_loss: 0.2216 - val_accuracy: 0.9410\n",
            "Epoch 47/50\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.0131 - accuracy: 0.9993 - val_loss: 0.2212 - val_accuracy: 0.9414\n",
            "Epoch 48/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.0122 - accuracy: 0.9994 - val_loss: 0.2242 - val_accuracy: 0.9413\n",
            "Epoch 49/50\n",
            "31/31 [==============================] - 1s 19ms/step - loss: 0.0111 - accuracy: 0.9995 - val_loss: 0.2313 - val_accuracy: 0.9400\n",
            "Epoch 50/50\n",
            "31/31 [==============================] - 1s 18ms/step - loss: 0.0102 - accuracy: 0.9997 - val_loss: 0.2316 - val_accuracy: 0.9398\n",
            "21/21 [==============================] - 1s 5ms/step\n",
            "F1 score: 0.7403110097363796\n"
          ]
        }
      ],
      "source": [
        "def grid_search(parameters, max_units):\n",
        "  baseline_units , layer_units = layer_unit_calculator(max_units)\n",
        "  if parameters[\"Additional_LSTM\"]:\n",
        "    for i in layer_units:\n",
        "      parameters[\"lstm_units\"] = i[0]\n",
        "      parameters[\"add_lstm_units\"] = i[1]\n",
        "      # Create an instance of the custom model\n",
        "      custom_model = CreateModel(config_dict).build(config_dict[\"max_sequence_length\"])\n",
        "\n",
        "      # Compile the model\n",
        "      custom_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "      # Print model summary\n",
        "      custom_model.summary()\n",
        "  elif parameters[\"Additional_Dense\"]:\n",
        "    for i in layer_units:\n",
        "      parameters[\"lstm_units\"] = i[0]\n",
        "      parameters[\"add_dense_units\"] = i[1]\n",
        "      # Create an instance of the custom model\n",
        "      custom_model = CreateModel(config_dict).build(config_dict[\"max_sequence_length\"])\n",
        "\n",
        "      # Compile the model\n",
        "      custom_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "      # Print model summary\n",
        "      custom_model.summary()\n",
        "  else:\n",
        "    for i in baseline_units:\n",
        "      parameters[\"lstm_units\"] = i\n",
        "      # Create an instance of the custom model\n",
        "      custom_model = CreateModel(config_dict).build(config_dict[\"max_sequence_length\"])\n",
        "      # Compile the model\n",
        "      custom_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "      # Print model summary\n",
        "      custom_model.summary()\n",
        "      history = custom_model.fit(text_train, y_train, batch_size=64, epochs=50, validation_data=(text_val,y_val) , verbose=1)\n",
        "      y_pred = custom_model.predict([text_test]).argmax(-1).flatten()\n",
        "      y_test_flatten = y_test.argmax(-1).flatten()\n",
        "      print(\"F1 score: {}\".format(f1_score(y_test_flatten, y_pred, labels=allowed_tokens, average='macro', zero_division=0)))\n",
        "\n",
        "\n",
        "\n",
        "config_dict = {\n",
        "    'vocab_size': 7405,\n",
        "    'embedding_dim': 100,\n",
        "    'max_sequence_length': max_sequence_length,\n",
        "    'embedding_matrix': embedding_matrix,\n",
        "    'tag_size': 46,\n",
        "    'lstm_units': 64,\n",
        "    'Additional_LSTM': False,\n",
        "    'Additional_Dense': False,\n",
        "    'add_lstm_units': None,\n",
        "    'add_dense_units': None\n",
        "}\n",
        "grid_search(config_dict,64)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hRTF5peW4l1"
      },
      "source": [
        "# [Task 5 - 1.0 points] Training and Evaluation\n",
        "\n",
        "You are now tasked to train and evaluate the Baseline, Model 1, and Model 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeKFSpzhW4l1"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Train **all** models on the train set.\n",
        "* Evaluate **all** models on the validation set.\n",
        "* Compute metrics on the validation set.\n",
        "* Pick **at least** three seeds for robust estimation.\n",
        "* Pick the **best** performing model according to the observed validation set performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KykofMUJW4l1"
      },
      "source": [
        "# [Task 6 - 1.0 points] Error Analysis\n",
        "\n",
        "You are tasked to evaluate your best performing model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4ZSpZfIW4l2"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Compare the errors made on the validation and test sets.\n",
        "* Aggregate model errors into categories (if possible)\n",
        "* Comment the about errors and propose possible solutions on how to address them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C99Cy5QDW4l2"
      },
      "source": [
        "# [Task 7 - 1.0 points] Report\n",
        "\n",
        "Wrap up your experiment in a short report (up to 2 pages)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-qlII0UW4l2"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Use the NLP course report template.\n",
        "* Summarize each task in the report following the provided template."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfJrNZ1dW4l2"
      },
      "source": [
        "### Recommendations\n",
        "\n",
        "The report is not a copy-paste of graphs, tables, and command outputs.\n",
        "\n",
        "* Summarize classification performance in Table format.\n",
        "* **Do not** report command outputs or screenshots.\n",
        "* Report learning curves in Figure format.\n",
        "* The error analysis section should summarize your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgoyCL54W4l2"
      },
      "source": [
        "# Submission\n",
        "\n",
        "* **Submit** your report in PDF format.\n",
        "* **Submit** your python notebook.\n",
        "* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc...\n",
        "* You can upload **model weights** in a cloud repository and report the link in the report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tidbmlVHW4l2"
      },
      "source": [
        "# FAQ\n",
        "\n",
        "Please check this frequently asked questions before contacting us"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBjCcgyTW4l2"
      },
      "source": [
        "### Execution Order\n",
        "\n",
        "You are **free** to address tasks in any order (if multiple orderings are available)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViqkGzmDW4l2"
      },
      "source": [
        "### Trainable Embeddings\n",
        "\n",
        "You are **free** to define a trainable or non-trainable Embedding layer to load the GloVe embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pUw7801W4l2"
      },
      "source": [
        "### Model architecture\n",
        "\n",
        "You **should not** change the architecture of a model (i.e., its layers).\n",
        "\n",
        "However, you are **free** to play with their hyper-parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAyHEwQqW4l2"
      },
      "source": [
        "### Neural Libraries\n",
        "\n",
        "You are **free** to use any library of your choice to implement the networks (e.g., Keras, Tensorflow, PyTorch, JAX, etc...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7eVgMsqW4l2"
      },
      "source": [
        "### Keras TimeDistributed Dense layer\n",
        "\n",
        "If you are using Keras, we recommend wrapping the final Dense layer with `TimeDistributed`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_mKQTaaW4l2"
      },
      "source": [
        "### Robust Evaluation\n",
        "\n",
        "Each model is trained with at least 3 random seeds.\n",
        "\n",
        "Task 4 requires you to compute the average performance over the 3 seeds and its corresponding standard deviation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIpgENGZW4l2"
      },
      "source": [
        "### Model Selection for Analysis\n",
        "\n",
        "To carry out the error analysis you are **free** to either\n",
        "\n",
        "* Pick examples or perform comparisons with an individual seed run model (e.g., Baseline seed 1337)\n",
        "* Perform ensembling via, for instance, majority voting to obtain a single model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGrB82VGW4l3"
      },
      "source": [
        "### Error Analysis\n",
        "\n",
        "Some topics for discussion include:\n",
        "   * Model performance on most/less frequent classes.\n",
        "   * Precision/Recall curves.\n",
        "   * Confusion matrices.\n",
        "   * Specific misclassified samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO4axOpbW4l3"
      },
      "source": [
        "### Punctuation\n",
        "\n",
        "**Do not** remove punctuation from documents since it may be helpful to the model.\n",
        "\n",
        "You should **ignore** it during metrics computation.\n",
        "\n",
        "If you are curious, you can run additional experiments to verify the impact of removing punctuation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg2LaahXW4l3"
      },
      "source": [
        "# The End"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}