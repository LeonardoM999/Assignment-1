{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WeCeITXoxLf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
    "\n",
    "**Keywords**: POS tagging, Sequence labelling, RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import urllib\n",
    "import sys\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "import tqdm\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "import os\n",
    "from typing import List, Callable, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_reproducibility(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    # os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\" ### can make training slower\n",
    "\n",
    "\n",
    "set_reproducibility(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 1 - 0.5 points] Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(download_path: Path, url: str):\n",
    "    urllib.request.urlretrieve(url, filename=download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded!\n",
      "Dataset already extracted!\n"
     ]
    }
   ],
   "source": [
    "dataset_url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "dataset_name = \"dependency_treebank\"\n",
    "\n",
    "# print(f\"Current work directory: {Path.cwd()}\")\n",
    "dataset_folder = Path.cwd().joinpath(\"Datasets\")\n",
    "if not dataset_folder.exists():\n",
    "    dataset_folder.mkdir(parents=True)\n",
    "\n",
    "dataset_zip_path = dataset_folder.joinpath(\"dependency_treebank.zip\")\n",
    "if not dataset_zip_path.exists():\n",
    "    print(\"Downloading dataset... \", end=\"\")\n",
    "    download_url(url=dataset_url, download_path=dataset_zip_path)\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(\"Dataset already downloaded!\")\n",
    "dataset_path = dataset_folder.joinpath(dataset_name)\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    print(\"Extracting dataset... (it may take a while...) \", end=\"\")\n",
    "    shutil.unpack_archive(dataset_zip_path, dataset_folder)\n",
    "    print(\"Extraction completed!\")\n",
    "else:\n",
    "    print(\"Dataset already extracted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the corpus into a pandas DataFrame object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = dataset_folder.joinpath(dataset_name)\n",
    "\n",
    "\n",
    "dataframe_rows = []\n",
    "for file_path in sorted(folder.glob(\"*.dp\")):\n",
    "    with file_path.open(mode=\"r\", encoding=\"utf-8\") as text_file:\n",
    "        # Reading the text\n",
    "        text = text_file.read()\n",
    "        # Split sentences (\\n\\n is used for most NLP datasets to split sentences)\n",
    "        sentences = text.split(\"\\n\\n\")\n",
    "\n",
    "        # Observing each sentence\n",
    "        for s in sentences:\n",
    "            sentence = []\n",
    "            tags = []\n",
    "            # sentence = [pierre,vinken,,aksjdajs, ]. tags = [NNP,aab,asd....]\n",
    "            # Taking every line\n",
    "            for line in s.split(\"\\n\"):\n",
    "                columns = line.split(\"\\t\")\n",
    "                # If every line have word, tag, value\n",
    "                if len(columns) > 2:\n",
    "                    # Put words and tags into lists\n",
    "                    sentence.append(columns[0])\n",
    "                    tags.append(columns[1])\n",
    "\n",
    "            # Get the File_ID\n",
    "            file_id = int(file_path.stem.split(\"_\")[1])\n",
    "            dataframe_row = {\"file_id\": file_id, \"sentence\": sentence, \"tag\": tags}\n",
    "            dataframe_rows.append(dataframe_row)\n",
    "# Create the dataframe\n",
    "df = pd.DataFrame(dataframe_rows)\n",
    "\n",
    "FILE_ID, SENTENCE, TAGS = df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[Pierre, Vinken, ,, 61, years, old, ,, will, j...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Mr., Vinken, is, chairman, of, Elsevier, N.V....</td>\n",
       "      <td>[NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[Rudolph, Agnew, ,, 55, years, old, and, forme...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[A, form, of, asbestos, once, used, to, make, ...</td>\n",
       "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>[The, asbestos, fiber, ,, crocidolite, ,, is, ...</td>\n",
       "      <td>[DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_id  ...                                                tag\n",
       "0        1  ...  [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...\n",
       "1        1  ...  [NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...\n",
       "2        2  ...  [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...\n",
       "3        3  ...  [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...\n",
       "4        3  ...  [DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Data Train-Test-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### file indices for train/validation/test\n",
    "train_ids = np.arange(1, 101)\n",
    "val_ids = np.arange(101, 151)\n",
    "test_ids = np.arange(151, 200)\n",
    "\n",
    "df_train = df[df[FILE_ID].isin(train_ids)]\n",
    "df_val = df[df[FILE_ID].isin(val_ids)]\n",
    "df_test = df[df[FILE_ID].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 2 - 0.5 points] Text encoding\n",
    "\n",
    "To train a neural POS tagger, you first need to encode text into numerical format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Embed words using **GloVe embeddings**.\n",
    "* You are **free** to pick any embedding dimension.\n",
    "* [Optional] You are free to experiment with text pre-processing: **make sure you do not delete any token!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "### download Glove\n",
    "\n",
    "glove_file = \"glove.6B.100d.txt\"\n",
    "glove_zip = \"glove.6B.zip\"\n",
    "glove_path = Path.cwd().joinpath(glove_zip)\n",
    "if not glove_path.exists():\n",
    "    urllib.request.urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", glove_zip)\n",
    "    with zipfile.ZipFile(glove_zip, \"r\") as zip_ref:\n",
    "        zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download glove\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO secondo me è meglio così\n",
    "# max_sequence_length = int(np.quantile([len(seq) for seq in df_train[\"sentence\"]], 0.99))\n",
    "max_sequence_length = np.max([len(seq) for seq in df_train[SENTENCE]])\n",
    "### a little\n",
    "\n",
    "# TODO remove\n",
    "hparams = {\n",
    "    \"batch_size\": 128,\n",
    "    \"embedding_dim\": 100,\n",
    "    \"embedding_trainable\": False,\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"max_sequence_length\": max_sequence_length,\n",
    "    \"vocab_size\": 7405,\n",
    "    \"tag_size\": 46,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing: lowercase TODO remove\n",
    "def make_lower(df):\n",
    "    df[SENTENCE] = df.map(lambda x: x.lower())\n",
    "\n",
    "\n",
    "make_lower(df_train)\n",
    "make_lower(df_val)\n",
    "# TODO va bene fare lowercase sul test set? vediamo se va senza\n",
    "# make_lower(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use Keras Tokenizer to create Vocabulary\n",
    "\n",
    "# ci hanno consigliato codice deprecato (tokenizer) io uso TextVectorization\n",
    "# dobbiamo paddare\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "### tokenization\n",
    "tokenizer = TextVectorization(\n",
    "    max_tokens=20000,  # TODO\n",
    "    output_sequence_length=max_sequence_length,\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    pad_to_max_tokens=True,  # TODO\n",
    ")\n",
    "ds_train = tf.data.Dataset.from_tensor_slices(df_train[SENTENCE].to_dict()).batch(\n",
    "    batch_size\n",
    ")\n",
    "tokenizer.adapt(ds_train)\n",
    "\n",
    "voc = tokenizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_tokenizer = TextVectorization(\n",
    "    max_tokens=100,\n",
    "    output_sequence_length=np.max([len(seq) for seq in df_train[TAGS]]),\n",
    "    standardize=None,\n",
    "    pad_to_max_tokens=False,\n",
    ")\n",
    "ds_train = tf.data.Dataset.from_tensor_slices(df_train[TAGS].to_dict()).batch(\n",
    "    batch_size\n",
    ")\n",
    "tag_tokenizer.adapt(ds_train)\n",
    "tag_size = len(tag_tokenizer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "# embedding_layer = Embedding(\n",
    "#    num_tokens,\n",
    "#    embedding_dim,\n",
    "#    trainable=False,\n",
    "# )\n",
    "# embedding_layer.build((1,))\n",
    "# embedding_layer.set_weights([embedding_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO capirci qualcosa, fare un po' di inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### categorical (one hot) encoding of the labels\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "num_classes = tag_size\n",
    "y_train = to_categorical(tag_train, num_classes)\n",
    "y_test = to_categorical(tag_test, num_classes)\n",
    "y_val = to_categorical(tag_val, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 3 - 1.0 points] Model definition\n",
    "\n",
    "You are now tasked to define your neural POS tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* **Baseline**: implement a Bidirectional LSTM with a Dense layer on top.\n",
    "* You are **free** to experiment with hyper-parameters to define the baseline model.\n",
    "\n",
    "* **Model 1**: add an additional LSTM layer to the Baseline model.\n",
    "* **Model 2**: add an additional Dense layer to the Baseline model.\n",
    "\n",
    "* **Do not mix Model 1 and Model 2**. Each model has its own instructions.\n",
    "\n",
    "**Note**: if a document contains many tokens, you are **free** to split them into chunks or sentences to define your mini-batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateModel(tf.keras.Model):\n",
    "    def __init__(self, lstm_units=64, add_lstm=False, add_dense=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # embedding_layer = Embedding(\n",
    "        #    num_tokens,\n",
    "        #    embedding_dim,\n",
    "        #    trainable=False,\n",
    "        # )\n",
    "        # embedding_layer.build((1,))\n",
    "        # embedding_layer.set_weights([embedding_matrix])\n",
    "        self.add_lstm = add_lstm\n",
    "        self.add_dense = add_dense\n",
    "\n",
    "        self.embedding_layer = Embedding(\n",
    "            num_tokens,\n",
    "            embedding_dim,\n",
    "            # input_length=max_sequence_length,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False,\n",
    "        )\n",
    "\n",
    "        self.bi_lstm = Bidirectional(LSTM(lstm_units, return_sequences=True))\n",
    "        self.bi_lstm2 = Bidirectional(LSTM(lstm_units, return_sequences=True))\n",
    "\n",
    "        self.dense2 = TimeDistributed(Dense(tag_size, activation=\"relu\"))\n",
    "        self.dense = TimeDistributed(Dense(tag_size, activation=\"softmax\"))\n",
    "\n",
    "    def call(self, input, training=False):\n",
    "        x = self.embedding_layer(input)\n",
    "        ### Model 1\n",
    "        if self.add_lstm:\n",
    "            x = self.bi_lstm(x)\n",
    "\n",
    "        ### Model 2\n",
    "        if self.add_dense:\n",
    "            x = self.dense2(x)\n",
    "\n",
    "        output = self.dense(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 4 - 1.0 points] Metrics\n",
    "\n",
    "Before training the models, you are tasked to define the evaluation metrics for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Evaluate your models using macro F1-score, compute over **all** tokens.\n",
    "* **Concatenate** all tokens in a data split to compute the F1-score. (**Hint**: accumulate FP, TP, FN, TN iteratively) \n",
    "* **Do not consider punctuation and symbol classes** $\\rightarrow$ [What is punctuation?](https://en.wikipedia.org/wiki/English_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Note**: What about OOV tokens?\n",
    "   * All the tokens in the **training** set that are not in GloVe are **not** considered as OOV\n",
    "   * For the remaining tokens (i.e., OOV in the validation and test sets), you have to assign them a **static** embedding.\n",
    "   * You are **free** to define the static embedding using any strategy (e.g., random, neighbourhood, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 5 - 1.0 points] Training and Evaluation\n",
    "\n",
    "You are now tasked to train and evaluate the Baseline, Model 1, and Model 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Train **all** models on the train set.\n",
    "* Evaluate **all** models on the validation set.\n",
    "* Compute metrics on the validation set.\n",
    "* Pick **at least** three seeds for robust estimation.\n",
    "* Pick the **best** performing model according to the observed validation set performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 6 - 1.0 points] Error Analysis\n",
    "\n",
    "You are tasked to evaluate your best performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Compare the errors made on the validation and test sets.\n",
    "* Aggregate model errors into categories (if possible) \n",
    "* Comment the about errors and propose possible solutions on how to address them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 7 - 1.0 points] Report\n",
    "\n",
    "Wrap up your experiment in a short report (up to 2 pages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Use the NLP course report template.\n",
    "* Summarize each task in the report following the provided template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recommendations\n",
    "\n",
    "The report is not a copy-paste of graphs, tables, and command outputs.\n",
    "\n",
    "* Summarize classification performance in Table format.\n",
    "* **Do not** report command outputs or screenshots.\n",
    "* Report learning curves in Figure format.\n",
    "* The error analysis section should summarize your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Submission\n",
    "\n",
    "* **Submit** your report in PDF format.\n",
    "* **Submit** your python notebook.\n",
    "* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc...\n",
    "* You can upload **model weights** in a cloud repository and report the link in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# FAQ\n",
    "\n",
    "Please check this frequently asked questions before contacting us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Trainable Embeddings\n",
    "\n",
    "You are **free** to define a trainable or non-trainable Embedding layer to load the GloVe embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model architecture\n",
    "\n",
    "You **should not** change the architecture of a model (i.e., its layers).\n",
    "\n",
    "However, you are **free** to play with their hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Neural Libraries\n",
    "\n",
    "You are **free** to use any library of your choice to implement the networks (e.g., Keras, Tensorflow, PyTorch, JAX, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Keras TimeDistributed Dense layer\n",
    "\n",
    "If you are using Keras, we recommend wrapping the final Dense layer with `TimeDistributed`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Error Analysis\n",
    "\n",
    "Some topics for discussion include:\n",
    "   * Model performance on most/less frequent classes.\n",
    "   * Precision/Recall curves.\n",
    "   * Confusion matrices.\n",
    "   * Specific misclassified samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Punctuation\n",
    "\n",
    "**Do not** remove punctuation from documents since it may be helpful to the model.\n",
    "\n",
    "You should **ignore** it during metrics computation.\n",
    "\n",
    "If you are curious, you can run additional experiments to verify the impact of removing punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
