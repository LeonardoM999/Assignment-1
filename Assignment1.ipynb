{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import urllib\n",
    "import sys\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "import tqdm\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "import os\n",
    "from typing import List, Callable, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(download_path: Path, url: str):\n",
    "    urllib.request.urlretrieve(url, filename=download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "dataset_name = \"dependency_treebank\"\n",
    "\n",
    "# print(f\"Current work directory: {Path.cwd()}\")\n",
    "dataset_folder = Path.cwd().joinpath(\"Datasets\")\n",
    "if not dataset_folder.exists():\n",
    "    dataset_folder.mkdir(parents=True)\n",
    "\n",
    "dataset_zip_path = dataset_folder.joinpath(\"dependency_treebank.zip\")\n",
    "if not dataset_zip_path.exists():\n",
    "    print(\"Downloading dataset... \", end=\"\")\n",
    "    download_url(url=dataset_url, download_path=dataset_zip_path)\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(\"Dataset already downloaded!\")\n",
    "dataset_path = dataset_folder.joinpath(dataset_name)\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    print(\"Extracting dataset... (it may take a while...) \", end=\"\")\n",
    "    shutil.unpack_archive(dataset_zip_path, dataset_folder)\n",
    "    print(\"Extraction completed!\")\n",
    "else:\n",
    "    print(\"Dataset already extracted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the corpus into a pandas DataFrame object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = dataset_folder.joinpath(dataset_name)\n",
    "\n",
    "\n",
    "dataframe_rows = []\n",
    "for file_path in sorted(folder.glob(\"*.dp\")):\n",
    "    with file_path.open(mode=\"r\", encoding=\"utf-8\") as text_file:\n",
    "        # Reading the text\n",
    "        text = text_file.read()\n",
    "        # Split sentences (\\n\\n is used for most NLP datasets to split sentences)\n",
    "        sentences = text.split(\"\\n\\n\")\n",
    "\n",
    "        # Observing each sentence\n",
    "        for s in sentences:\n",
    "            sentence = []\n",
    "            tags = []\n",
    "            # sentence = [pierre,vinken,,aksjdajs, ]. tags = [NNP,aab,asd....]\n",
    "            # Taking every line\n",
    "            for line in s.split(\"\\n\"):\n",
    "                columns = line.split(\"\\t\")\n",
    "                # If every line have word, tag, value\n",
    "                if len(columns) > 2:\n",
    "                    # Put words and tags into lists\n",
    "                    sentence.append(columns[0])\n",
    "                    tags.append(columns[1])\n",
    "\n",
    "            # Get the File_ID\n",
    "            file_id = int(file_path.stem.split(\"_\")[1])\n",
    "            dataframe_row = {\"file_id\": file_id, \"sentence\": sentence, \"tag\": tags}\n",
    "            dataframe_rows.append(dataframe_row)\n",
    "# Create the dataframe\n",
    "df = pd.DataFrame(dataframe_rows)\n",
    "\n",
    "FILE_ID, WORD, TAG = df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Data Train-Test-Validation\n",
    "Before splitting, lower case convertion is done as a mini preprocessing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make a list lowercase\n",
    "def lowercase_list(input_list):\n",
    "    return [item.lower() for item in input_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentence\"] = df[\"sentence\"].apply(lowercase_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### file indices for train/validation/test\n",
    "train_ids = np.arange(1, 101)\n",
    "val_ids = np.arange(101, 151)\n",
    "test_ids = np.arange(151, 200)\n",
    "\n",
    "df_train = df[df[FILE_ID].isin(train_ids)]\n",
    "df_val = df[df[FILE_ID].isin(val_ids)]\n",
    "df_test = df[df[FILE_ID].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Text encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_reproducibility(seed):\n",
    "    random.seed(seed)  # Seed for the Python built-in random module\n",
    "    np.random.seed(seed)  # Seed for NumPy\n",
    "    tf.random.set_seed(seed)  # Seed for TensorFlow\n",
    "    os.environ[\n",
    "        \"TF_DETERMINISTIC_OPS\"\n",
    "    ] = \"1\"  # Set an environment variable for deterministic TensorFlow operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters for Embedding and First Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = int(np.quantile([len(seq) for seq in df_train[\"sentence\"]], 0.99))\n",
    "hparams = {\n",
    "    \"batch_size\": 128,\n",
    "    \"embedding_dim\": 100,\n",
    "    \"embedding_trainable\": False,\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"max_sequence_length\": max_sequence_length,\n",
    "    \"vocab_size\": 7405,\n",
    "    \"tag_size\": 46,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary Creation & Tokenization\n",
    "Keras Tokenizer Class is used for tokenization and vocabulary creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use Keras Tokenizer to create Vocabulary\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"OOV\")\n",
    "tokenizer.fit_on_texts(df_train[\"sentence\"])\n",
    "\n",
    "tag_tokenizer = Tokenizer()\n",
    "tag_tokenizer.fit_on_texts(df_train[\"tag\"])\n",
    "\n",
    "\n",
    "# Turns text into into padded sequences.\n",
    "def prep_text(texts, tokenizer, max_sequence_length):\n",
    "    text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return sequence.pad_sequences(\n",
    "        text_sequences, maxlen=max_sequence_length, padding=\"post\"\n",
    "    )\n",
    "\n",
    "\n",
    "text_train = prep_text(df_train[\"sentence\"], tokenizer, hparams[\"max_sequence_length\"])\n",
    "text_test = prep_text(df_test[\"sentence\"], tokenizer, hparams[\"max_sequence_length\"])\n",
    "text_val = prep_text(df_val[\"sentence\"], tokenizer, hparams[\"max_sequence_length\"])\n",
    "\n",
    "tag_train = prep_text(df_train[\"tag\"], tag_tokenizer, hparams[\"max_sequence_length\"])\n",
    "tag_test = prep_text(df_test[\"tag\"], tag_tokenizer, hparams[\"max_sequence_length\"])\n",
    "tag_val = prep_text(df_val[\"tag\"], tag_tokenizer, hparams[\"max_sequence_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot-Encoding\n",
    "One-Hode-Encoding is done to use it in training and evaluation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "num_classes = len(tag_tokenizer.word_index) + 1\n",
    "y_train = to_categorical(tag_train, num_classes)\n",
    "y_test = to_categorical(tag_test, num_classes)\n",
    "y_val = to_categorical(tag_val, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tag-Tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes = list(tag_tokenizer.word_index.keys())\n",
    "all_tokens = list(tag_tokenizer.word_index.values())\n",
    "punct_classes = [\",\", \".\", \":\", \"``\", \"''\", \"$\", \"#\", \"sym\", \"-rrb-\", \"-lrb-\"]\n",
    "punct_tokens = [tag_tokenizer.word_index[p] for p in punct_classes]\n",
    "allowed_classes = [\n",
    "    word for word in tag_tokenizer.index_word.values() if word not in punct_classes\n",
    "]\n",
    "allowed_tokens = [token for token in all_tokens if token not in punct_tokens]\n",
    "\n",
    "print(\n",
    "    f\"Tags: {all_classes}\\n\"\n",
    "    + f\"All tag-tokens: {all_tokens}\\n\\n\"\n",
    "    + f\"Punctuations: {punct_classes}\\n\"\n",
    "    + f\"Tokenized punctuations {punct_tokens}\\n\\n\"\n",
    "    + f\"Tags without punctuation: {allowed_classes}\\n\"\n",
    "    + f\"Tokens will be used in evaluations: {allowed_tokens}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading Pre-Trained Glove Embeddings\n",
    "This may take a few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "zip_file = urllib.request.urlopen(zip_file_url)\n",
    "archive = zipfile.ZipFile(io.BytesIO(zip_file.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Embedding Matrix\n",
    "Downloaded GloVe embeddings were used to create an embedding matrix, where the rows contain the word embeddings for the tokens in the Tokenizer's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "glove_file = \"glove.6B.100d.txt\"\n",
    "\n",
    "with archive.open(glove_file) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0].decode(\"utf-8\")\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, hparams[\"embedding_dim\"]))\n",
    "num_words_in_embedding = 0\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        num_words_in_embedding += 1\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inspect tokens' embedding vectors\n",
    "idx_token = 2\n",
    "print(\n",
    "    f\"Token: {list(tokenizer.word_index.keys())[idx_token]} \\nVector: {embedding_matrix[idx_token]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Task 3 - 1.0 points] Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "Keras Subclassing method was used while creating the model.\n",
    "\n",
    "\n",
    "An instance of the CreateModel class represents the model, and its architecture is specified based on the configuration provided.\n",
    "\n",
    "The model includes:\n",
    "* An Embedding Layer\n",
    "* A Bidirectional LSTM Layer\n",
    "* Optional Bidirectional LSTM Layer\n",
    "* Optional dense layer.\n",
    "* Time-distributed Dense Layer\n",
    "\n",
    "It generates output sequences using a time-distributed dense layer. The architecture is configured through parameters such as vocabulary size, embedding dimension, LSTM units, and additional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM,\n",
    "    Embedding,\n",
    "    Dense,\n",
    "    TimeDistributed,\n",
    "    Dropout,\n",
    "    Bidirectional,\n",
    "    Input,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class CreateModel(tf.keras.Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        vocab_size = config[\"vocab_size\"]\n",
    "        embedding_dim = config[\"embedding_dim\"]\n",
    "        max_sequence_length = config[\"max_sequence_length\"]\n",
    "        embedding_matrix = config[\"embedding_matrix\"]\n",
    "        tag_size = config[\"tag_size\"]\n",
    "        lstm_units = config[\"lstm_units\"]\n",
    "        Additional_LSTM = config[\"Additional_LSTM\"]\n",
    "        Additional_Dense = config[\"Additional_Dense\"]\n",
    "        add_lstm_units = config[\"add_lstm_units\"]\n",
    "        add_dense_units = config[\"add_dense_units\"]\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding_layer = Embedding(\n",
    "            vocab_size + 1,\n",
    "            embedding_dim,\n",
    "            input_length=max_sequence_length,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False,\n",
    "        )\n",
    "\n",
    "        # Bidirectional LSTM layer\n",
    "        self.bi_lstm = Bidirectional(LSTM(lstm_units, return_sequences=True))\n",
    "        # Additional LSTM\n",
    "        self.additional_lstm = (\n",
    "            Bidirectional(LSTM(add_lstm_units, return_sequences=True))\n",
    "            if Additional_LSTM\n",
    "            else None\n",
    "        )\n",
    "        # Additional Dense\n",
    "        self.additional_dense = (\n",
    "            Dense(add_dense_units, activation=\"relu\") if Additional_Dense else None\n",
    "        )\n",
    "\n",
    "        # Dense output layer\n",
    "        self.dense_output = TimeDistributed(Dense(tag_size, activation=\"softmax\"))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Define the forward pass\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.bi_lstm(x)\n",
    "\n",
    "        # Add the additional LSTM layer if specified\n",
    "        if self.additional_lstm:\n",
    "            x = self.additional_lstm(x)\n",
    "\n",
    "        # Add the additional Dense layer if specified\n",
    "        if self.additional_dense:\n",
    "            x = self.additional_dense(x)\n",
    "\n",
    "        outputs = self.dense_output(x)\n",
    "        return outputs\n",
    "\n",
    "    def build(self, shape):\n",
    "        x = tf.keras.layers.Input(shape=(shape,))\n",
    "        return tf.keras.Model(inputs=x, outputs=self.call(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 56)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 56, 100)           740600    \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 56, 128)           84480     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 56, 46)            5934      \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 831014 (3.17 MB)\n",
      "Trainable params: 90414 (353.18 KB)\n",
      "Non-trainable params: 740600 (2.83 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "config_dict = {\n",
    "    \"vocab_size\": 7405,\n",
    "    \"embedding_dim\": 100,\n",
    "    \"max_sequence_length\": max_sequence_length,\n",
    "    \"embedding_matrix\": embedding_matrix,\n",
    "    \"tag_size\": 46,\n",
    "    \"lstm_units\": 64,\n",
    "    \"Additional_LSTM\": False,\n",
    "    \"Additional_Dense\": False,\n",
    "    \"add_lstm_units\": None,\n",
    "    \"add_dense_units\": None,\n",
    "}\n",
    "# Create an instance of the custom model\n",
    "custom_model = CreateModel(config_dict).build(config_dict[\"max_sequence_length\"])\n",
    "\n",
    "# Compile the model\n",
    "custom_model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "\n",
    "custom_model.summary()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
