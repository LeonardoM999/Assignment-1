{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WeCeITXoxLf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
    "\n",
    "**Keywords**: POS tagging, Sequence labelling, RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 00:35:40.008261: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-08 00:35:40.055054: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-08 00:35:40.056018: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-08 00:35:40.873782: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# TODO remove\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import urllib\n",
    "import sys\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "import tqdm\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "import os\n",
    "from typing import List, Callable, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_reproducibility(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    # os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\" ### can make training slower\n",
    "\n",
    "\n",
    "set_reproducibility(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 1 - 0.5 points] Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(download_path: Path, url: str):\n",
    "    urllib.request.urlretrieve(url, filename=download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded!\n",
      "Dataset already extracted!\n"
     ]
    }
   ],
   "source": [
    "dataset_url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "dataset_name = \"dependency_treebank\"\n",
    "\n",
    "# print(f\"Current work directory: {Path.cwd()}\")\n",
    "dataset_folder = Path.cwd().joinpath(\"Datasets\")\n",
    "if not dataset_folder.exists():\n",
    "    dataset_folder.mkdir(parents=True)\n",
    "\n",
    "dataset_zip_path = dataset_folder.joinpath(\"dependency_treebank.zip\")\n",
    "if not dataset_zip_path.exists():\n",
    "    print(\"Downloading dataset... \", end=\"\")\n",
    "    download_url(url=dataset_url, download_path=dataset_zip_path)\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(\"Dataset already downloaded!\")\n",
    "dataset_path = dataset_folder.joinpath(dataset_name)\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    print(\"Extracting dataset... (it may take a while...) \", end=\"\")\n",
    "    shutil.unpack_archive(dataset_zip_path, dataset_folder)\n",
    "    print(\"Extraction completed!\")\n",
    "else:\n",
    "    print(\"Dataset already extracted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the corpus into a pandas DataFrame object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = dataset_folder.joinpath(dataset_name)\n",
    "\n",
    "\n",
    "dataframe_rows = []\n",
    "for file_path in sorted(folder.glob(\"*.dp\")):\n",
    "    with file_path.open(mode=\"r\", encoding=\"utf-8\") as text_file:\n",
    "        # Reading the text\n",
    "        text = text_file.read()\n",
    "        # Split sentences (\\n\\n is used for most NLP datasets to split sentences)\n",
    "        sentences = text.split(\"\\n\\n\")\n",
    "\n",
    "        # Observing each sentence\n",
    "        for s in sentences:\n",
    "            sentence = []\n",
    "            tags = []\n",
    "            # sentence = [pierre,vinken,,aksjdajs, ]. tags = [NNP,aab,asd....]\n",
    "            # Taking every line\n",
    "            for line in s.split(\"\\n\"):\n",
    "                columns = line.split(\"\\t\")\n",
    "                # If every line have word, tag, value\n",
    "                if len(columns) > 2:\n",
    "                    # Put words and tags into lists\n",
    "                    sentence.append(columns[0])\n",
    "                    tags.append(columns[1])\n",
    "\n",
    "            # Get the File_ID\n",
    "            file_id = int(file_path.stem.split(\"_\")[1])\n",
    "            dataframe_row = {\"file_id\": file_id, \"sentence\": sentence, \"tag\": tags}\n",
    "            dataframe_rows.append(dataframe_row)\n",
    "# Create the dataframe\n",
    "df = pd.DataFrame(dataframe_rows)\n",
    "\n",
    "FILE_ID, SENTENCE, TAGS = df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[Pierre, Vinken, ,, 61, years, old, ,, will, j...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Mr., Vinken, is, chairman, of, Elsevier, N.V....</td>\n",
       "      <td>[NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[Rudolph, Agnew, ,, 55, years, old, and, forme...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[A, form, of, asbestos, once, used, to, make, ...</td>\n",
       "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>[The, asbestos, fiber, ,, crocidolite, ,, is, ...</td>\n",
       "      <td>[DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_id                                           sentence  \\\n",
       "0        1  [Pierre, Vinken, ,, 61, years, old, ,, will, j...   \n",
       "1        1  [Mr., Vinken, is, chairman, of, Elsevier, N.V....   \n",
       "2        2  [Rudolph, Agnew, ,, 55, years, old, and, forme...   \n",
       "3        3  [A, form, of, asbestos, once, used, to, make, ...   \n",
       "4        3  [The, asbestos, fiber, ,, crocidolite, ,, is, ...   \n",
       "\n",
       "                                                 tag  \n",
       "0  [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...  \n",
       "1  [NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...  \n",
       "2  [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...  \n",
       "3  [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...  \n",
       "4  [DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Data Train-Test-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### file indices for train/validation/test\n",
    "train_ids = np.arange(1, 101)\n",
    "val_ids = np.arange(101, 151)\n",
    "test_ids = np.arange(151, 200)\n",
    "\n",
    "df_train = df[df[FILE_ID].isin(train_ids)]\n",
    "df_val = df[df[FILE_ID].isin(val_ids)]\n",
    "df_test = df[df[FILE_ID].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 2 - 0.5 points] Text encoding\n",
    "\n",
    "To train a neural POS tagger, you first need to encode text into numerical format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Embed words using **GloVe embeddings**.\n",
    "* You are **free** to pick any embedding dimension.\n",
    "* [Optional] You are free to experiment with text pre-processing: **make sure you do not delete any token!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "### download Glove\n",
    "\n",
    "embedding_dim = 100\n",
    "glove_file = f\"glove.6B.{embedding_dim}d.txt\"\n",
    "glove_zip = \"glove.6B.zip\"\n",
    "glove_path = Path.cwd().joinpath(glove_file)\n",
    "if not glove_path.exists():\n",
    "    urllib.request.urlretrieve(\n",
    "        \"http://nlp.stanford.edu/data/glove.6B.zip\", Path.cwd().joinpath(glove_zip)\n",
    "    )\n",
    "    with zipfile.ZipFile(glove_zip, \"r\") as zip_ref:\n",
    "        zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_sequence_length = int(np.quantile([len(seq) for seq in df_train[\"sentence\"]], 0.99))\n",
    "max_sequence_length = max([len(seq) for seq in df_train[SENTENCE]])\n",
    "\n",
    "# TODO remove\n",
    "hparams = {\n",
    "    \"batch_size\": 128,\n",
    "    \"embedding_dim\": 100,\n",
    "    \"embedding_trainable\": False,\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"max_sequence_length\": max_sequence_length,\n",
    "    \"vocab_size\": 7405,\n",
    "    \"tag_size\": 46,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing: lowercase\n",
    "def make_lower(df):\n",
    "    df[SENTENCE].apply(lambda l: [i.lower() for i in l])\n",
    "\n",
    "\n",
    "make_lower(df_train)\n",
    "make_lower(df_val)\n",
    "# TODO va bene fare lowercase sul test set? vediamo se va senza\n",
    "# make_lower(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use Keras Tokenizer to create Vocabulary\n",
    "\n",
    "# TODO ci hanno consigliato codice deprecato (tokenizer) io vorrei usare TextVectorization ma fa schifo\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"OOV\")\n",
    "tokenizer.fit_on_texts(df_train[\"sentence\"])\n",
    "\n",
    "tag_tokenizer = Tokenizer()\n",
    "tag_tokenizer.fit_on_texts(df_train[\"tag\"])\n",
    "\n",
    "\n",
    "### Turn text into into padded sequences.\n",
    "def prep_text(texts, tokenizer, max_sequence_length):\n",
    "    text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return sequence.pad_sequences(\n",
    "        text_sequences, maxlen=max_sequence_length, padding=\"post\"\n",
    "    )\n",
    "\n",
    "\n",
    "text_train = prep_text(df_train[\"sentence\"], tokenizer, max_sequence_length)\n",
    "text_test = prep_text(df_test[\"sentence\"], tokenizer, max_sequence_length)\n",
    "text_val = prep_text(df_val[\"sentence\"], tokenizer, max_sequence_length)\n",
    "\n",
    "tag_train = prep_text(df_train[\"tag\"], tag_tokenizer, max_sequence_length)\n",
    "tag_test = prep_text(df_test[\"tag\"], tag_tokenizer, max_sequence_length)\n",
    "tag_val = prep_text(df_val[\"tag\"], tag_tokenizer, max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  2,  7, 11,  5,  6,  7, 20, 12,  4,  1,  3,  4,  6,  1,  2, 11,\n",
       "        8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_train.shape\n",
    "tag_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "### encode the tags in 1hot encoding\n",
    "\n",
    "num_classes = len(tag_tokenizer.word_index) + 1\n",
    "tag_categorical_train = to_categorical(tag_train, num_classes)\n",
    "tag_categorical_test = to_categorical(tag_test, num_classes)\n",
    "tag_categorical_val = to_categorical(tag_val, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags: ['nn', 'nnp', 'in', 'dt', 'nns', 'jj', ',', '.', 'vbd', 'rb', 'cd', 'vb', 'cc', 'vbz', 'vbn', 'to', 'prp', 'vbg', 'vbp', 'md', 'prp$', '``', 'pos', \"''\", '$', ':', 'wdt', 'jjr', 'wp', 'rp', 'nnps', 'jjs', 'wrb', 'rbr', '-rrb-', '-lrb-', 'ex', 'rbs', 'ls', 'pdt', 'wp$', 'fw', 'uh', 'sym', '#']\n",
      "All tag-tokens: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]\n",
      "\n",
      "Punctuations: [',', '.', ':', '``', \"''\", '$', '#', 'sym', '-rrb-', '-lrb-']\n",
      "Tokenized punctuations [7, 8, 26, 22, 24, 25, 45, 44, 35, 36]\n",
      "\n",
      "Tags without punctuation: ['nn', 'nnp', 'in', 'dt', 'nns', 'jj', 'vbd', 'rb', 'cd', 'vb', 'cc', 'vbz', 'vbn', 'to', 'prp', 'vbg', 'vbp', 'md', 'prp$', 'pos', 'wdt', 'jjr', 'wp', 'rp', 'nnps', 'jjs', 'wrb', 'rbr', 'ex', 'rbs', 'ls', 'pdt', 'wp$', 'fw', 'uh']\n",
      "Tokens will be used in evaluations: [1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 27, 28, 29, 30, 31, 32, 33, 34, 37, 38, 39, 40, 41, 42, 43]\n"
     ]
    }
   ],
   "source": [
    "all_classes = list(tag_tokenizer.word_index.keys())\n",
    "all_tokens = list(tag_tokenizer.word_index.values())\n",
    "punct_classes = [\",\", \".\", \":\", \"``\", \"''\", \"$\", \"#\", \"sym\", \"-rrb-\", \"-lrb-\"]\n",
    "punct_tokens = [tag_tokenizer.word_index[p] for p in punct_classes]\n",
    "allowed_classes = [\n",
    "    word for word in tag_tokenizer.index_word.values() if word not in punct_classes\n",
    "]\n",
    "allowed_tokens = [token for token in all_tokens if token not in punct_tokens]\n",
    "\n",
    "print(f\"Tags: {all_classes}\")\n",
    "print(f\"All tag-tokens: {all_tokens}\\n\")\n",
    "print(f\"Punctuations: {punct_classes}\")\n",
    "print(f\"Tokenized punctuations {punct_tokens}\\n\")\n",
    "print(f\"Tags without punctuation: {allowed_classes}\")\n",
    "print(f\"Tokens will be used in evaluations: {allowed_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "### Create the embedding matrix\n",
    "embeddings_index = {}\n",
    "with open(glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(f\"Found {len(embeddings_index)} word vectors.\")\n",
    "\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "num_words_in_embedding = 0\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        num_words_in_embedding += 1\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: the \n",
      "Vector: [-0.10767     0.11053     0.59811997 -0.54360998  0.67395997  0.10663\n",
      "  0.038867    0.35481     0.06351    -0.094189    0.15786    -0.81664997\n",
      "  0.14172     0.21939     0.58504999 -0.52157998  0.22782999 -0.16642\n",
      " -0.68228     0.35870001  0.42568001  0.19021     0.91962999  0.57555002\n",
      "  0.46184999  0.42363    -0.095399   -0.42749    -0.16566999 -0.056842\n",
      " -0.29595     0.26036999 -0.26605999 -0.070404   -0.27662     0.15820999\n",
      "  0.69825     0.43081     0.27952    -0.45436999 -0.33801001 -0.58183998\n",
      "  0.22363999 -0.57779998 -0.26862001 -0.20424999  0.56393999 -0.58524001\n",
      " -0.14365    -0.64218003  0.0054697  -0.35247999  0.16162001  1.1796\n",
      " -0.47674    -2.75530005 -0.1321     -0.047729    1.06550002  1.10339999\n",
      " -0.2208      0.18669     0.13177     0.15117     0.71310002 -0.35214999\n",
      "  0.91347998  0.61782998  0.70991999  0.23954999 -0.14571001 -0.37858999\n",
      " -0.045959   -0.47367999  0.2385      0.20536    -0.18996     0.32506999\n",
      " -1.11119998 -0.36341     0.98679    -0.084776   -0.54008001  0.11726\n",
      " -1.0194     -0.24424     0.12771     0.013884    0.080374   -0.35414001\n",
      "  0.34951001 -0.72259998  0.37549001  0.44409999 -0.99058998  0.61214\n",
      " -0.35111001 -0.83155     0.45293     0.082577  ]\n"
     ]
    }
   ],
   "source": [
    "### Inspect tokens' embedding vectors\n",
    "idx_token = 2\n",
    "print(\n",
    "    f\"Token: {list(tokenizer.word_index.keys())[idx_token]} \\nVector: {embedding_matrix[idx_token]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fare un po' di inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Input\n",
    "\n",
    "# embedding_layer = Embedding(\n",
    "#    num_tokens,\n",
    "#    embedding_dim,\n",
    "#    trainable=False,\n",
    "# )\n",
    "# embedding_layer.build((1,))\n",
    "# embedding_layer.set_weights([embedding_matrix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 3 - 1.0 points] Model definition\n",
    "\n",
    "You are now tasked to define your neural POS tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* **Baseline**: implement a Bidirectional LSTM with a Dense layer on top.\n",
    "* You are **free** to experiment with hyper-parameters to define the baseline model.\n",
    "\n",
    "* **Model 1**: add an additional LSTM layer to the Baseline model.\n",
    "* **Model 2**: add an additional Dense layer to the Baseline model.\n",
    "\n",
    "* **Do not mix Model 1 and Model 2**. Each model has its own instructions.\n",
    "\n",
    "**Note**: if a document contains many tokens, you are **free** to split them into chunks or sentences to define your mini-batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, TimeDistributed\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateModel(tf.keras.Model):\n",
    "    def __init__(self, lstm_units=64, add_lstm=False, add_dense=False):\n",
    "        super().__init__()\n",
    "\n",
    "        number_of_words = len(tokenizer.word_index.keys())\n",
    "        number_of_tags = len(tag_tokenizer.world_index.keys())\n",
    "\n",
    "        self.embedding_layer = Embedding(\n",
    "            input_dim=number_of_words,\n",
    "            output_dim=number_of_tags,\n",
    "            input_length=max_sequence_length,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False,\n",
    "        )\n",
    "\n",
    "        self.bi_lstm = Bidirectional(LSTM(lstm_units, return_sequences=True))\n",
    "        self.bi_lstm2 = Bidirectional(LSTM(lstm_units, return_sequences=True))\n",
    "\n",
    "        self.dense2 = TimeDistributed(Dense(number_of_tags, activation=\"relu\"))\n",
    "        self.dense = TimeDistributed(Dense(number_of_tags, activation=\"softmax\"))\n",
    "\n",
    "    def call(self, input, training=False):\n",
    "        x = self.embedding_layer(input)\n",
    "        ### Model 1\n",
    "        if self.add_lstm:\n",
    "            x = self.bi_lstm(x)\n",
    "\n",
    "        ### Model 2\n",
    "        if self.add_dense:\n",
    "            x = self.dense2(x)\n",
    "\n",
    "        output = self.dense(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(lstm_units=64, add_lstm=False, add_dense=False):\n",
    "    number_of_words = len(tokenizer.word_index.keys())\n",
    "    number_of_tags = len(tag_tokenizer.world_index.keys())\n",
    "\n",
    "    input = Input(shape=(max_sequence_length,))\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=number_of_words,\n",
    "        output_dim=number_of_tags,\n",
    "        input_length=max_sequence_length,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=False,\n",
    "    )(input)\n",
    "    bi_lstm = Bidirectional(LSTM(lstm_units, return_sequences=True))(input)\n",
    "    bi_lstm2 = (\n",
    "        Bidirectional(LSTM(lstm_units, return_sequences=True))(bi_lstm)\n",
    "        if add_lstm\n",
    "        else bi_lstm\n",
    "    )\n",
    "    dense2 = (\n",
    "        TimeDistributed(Dense(number_of_tags, activation=\"relu\"))(bi_lstm)\n",
    "        if add_dense\n",
    "        else bi_lstm\n",
    "    )\n",
    "    dense_output = TimeDistributed(Dense(number_of_tags, activation=\"softmax\"))(bi_lstm)\n",
    "    model = Model(input, dense_output)\n",
    "\n",
    "    # TODO from_logits=True è corretto?\n",
    "    model.compile(\n",
    "        loss=SparseCategoricalCrossentropy(from_logits=True), optimizer=Adam(1e-3)\n",
    "    )\n",
    "    model.summary()\n",
    "    return model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"f1_score\", mode=\"max\", patience=3, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 4 - 1.0 points] Metrics\n",
    "\n",
    "Before training the models, you are tasked to define the evaluation metrics for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Evaluate your models using macro F1-score, compute over **all** tokens.\n",
    "* **Concatenate** all tokens in a data split to compute the F1-score. (**Hint**: accumulate FP, TP, FN, TN iteratively) \n",
    "* **Do not consider punctuation and symbol classes** $\\rightarrow$ [What is punctuation?](https://en.wikipedia.org/wiki/English_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Note**: What about OOV tokens?\n",
    "   * All the tokens in the **training** set that are not in GloVe are **not** considered as OOV\n",
    "   * For the remaining tokens (i.e., OOV in the validation and test sets), you have to assign them a **static** embedding.\n",
    "   * You are **free** to define the static embedding using any strategy (e.g., random, neighbourhood, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(model):\n",
    "    # TODO what is a static embedding?\n",
    "\n",
    "    # TODO controllare che questo vada bene: fare la prova accumulando manualmente TP FP TN FN\n",
    "\n",
    "    y_pred = model.predict([text_val]).argmax(-1).flatten()\n",
    "    y_test_flatten = tag_categorical_val.argmax(-1).flatten()\n",
    "    score = f1_score(\n",
    "        y_test_flatten,\n",
    "        y_pred,\n",
    "        labels=allowed_tokens,\n",
    "        average=\"macro\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 5 - 1.0 points] Training and Evaluation\n",
    "\n",
    "You are now tasked to train and evaluate the Baseline, Model 1, and Model 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Train **all** models on the train set.\n",
    "* Evaluate **all** models on the validation set.\n",
    "* Compute metrics on the validation set.\n",
    "* Pick **at least** three seeds for robust estimation.\n",
    "* Pick the **best** performing model according to the observed validation set performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(seed, model1=False, model2=False):\n",
    "    set_reproducibility(seed)\n",
    "\n",
    "    model = get_model(add_lstm=model1, add_dense=model2)\n",
    "\n",
    "    ### train model on the train set\n",
    "    history = model.fit(\n",
    "        text_train,\n",
    "        tag_categorical_train,\n",
    "        batch_size=64,\n",
    "        epochs=50,\n",
    "        validation_data=(text_val, tag_categorical_val),\n",
    "        callbacks=[callback],\n",
    "    )\n",
    "\n",
    "    ### compute metrics\n",
    "    score = metric(model)\n",
    "\n",
    "    return score, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [333, 666, 999]\n",
    "\n",
    "# TODO forse con dei dizionari viene meglio...\n",
    "\n",
    "### baseline\n",
    "baseline_results = [experiment(s) for s in seeds]\n",
    "baseline_scores = [r[0] for r in baseline_results]\n",
    "baseline_histories = [r[1] for r in baseline_results]\n",
    "\n",
    "### model1\n",
    "model1_results = [experiment(s, model1=True) for s in seeds]\n",
    "model1_scores = [r[0] for r in model1_results]\n",
    "model1_histories = [r[1] for r in model1_results]\n",
    "\n",
    "### model2\n",
    "model2_results = [experiment(s, model2=True) for s in seeds]\n",
    "model2_scores = [r[0] for r in model2_results]\n",
    "model2_histories = [r[1] for r in model2_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 6 - 1.0 points] Error Analysis\n",
    "\n",
    "You are tasked to evaluate your best performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Compare the errors made on the validation and test sets.\n",
    "* Aggregate model errors into categories (if possible) \n",
    "* Comment the about errors and propose possible solutions on how to address them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 7 - 1.0 points] Report\n",
    "\n",
    "Wrap up your experiment in a short report (up to 2 pages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Use the NLP course report template.\n",
    "* Summarize each task in the report following the provided template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recommendations\n",
    "\n",
    "The report is not a copy-paste of graphs, tables, and command outputs.\n",
    "\n",
    "* Summarize classification performance in Table format.\n",
    "* **Do not** report command outputs or screenshots.\n",
    "* Report learning curves in Figure format.\n",
    "* The error analysis section should summarize your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Submission\n",
    "\n",
    "* **Submit** your report in PDF format.\n",
    "* **Submit** your python notebook.\n",
    "* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc...\n",
    "* You can upload **model weights** in a cloud repository and report the link in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# FAQ\n",
    "\n",
    "Please check this frequently asked questions before contacting us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Trainable Embeddings\n",
    "\n",
    "You are **free** to define a trainable or non-trainable Embedding layer to load the GloVe embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model architecture\n",
    "\n",
    "You **should not** change the architecture of a model (i.e., its layers).\n",
    "\n",
    "However, you are **free** to play with their hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Neural Libraries\n",
    "\n",
    "You are **free** to use any library of your choice to implement the networks (e.g., Keras, Tensorflow, PyTorch, JAX, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Keras TimeDistributed Dense layer\n",
    "\n",
    "If you are using Keras, we recommend wrapping the final Dense layer with `TimeDistributed`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Error Analysis\n",
    "\n",
    "Some topics for discussion include:\n",
    "   * Model performance on most/less frequent classes.\n",
    "   * Precision/Recall curves.\n",
    "   * Confusion matrices.\n",
    "   * Specific misclassified samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Punctuation\n",
    "\n",
    "**Do not** remove punctuation from documents since it may be helpful to the model.\n",
    "\n",
    "You should **ignore** it during metrics computation.\n",
    "\n",
    "If you are curious, you can run additional experiments to verify the impact of removing punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
