{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WeCeITXoxLf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
    "\n",
    "**Keywords**: POS tagging, Sequence labelling, RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 11:39:44.757922: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-09 11:39:44.804225: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-09 11:39:44.805065: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-09 11:39:45.576512: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# TODO remove\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "import tqdm\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, TimeDistributed\n",
    "from tensorflow.keras.losses import (\n",
    "    SparseCategoricalCrossentropy,\n",
    "    CategoricalCrossentropy,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_reproducibility(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    # os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\" ### can make training slower\n",
    "\n",
    "\n",
    "set_reproducibility(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 1 - 0.5 points] Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(download_path: Path, url: str):\n",
    "    urllib.request.urlretrieve(url, filename=download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded!\n",
      "Dataset already extracted!\n"
     ]
    }
   ],
   "source": [
    "dataset_url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "dataset_name = \"dependency_treebank\"\n",
    "\n",
    "# print(f\"Current work directory: {Path.cwd()}\")\n",
    "dataset_folder = Path.cwd().joinpath(\"Datasets\")\n",
    "if not dataset_folder.exists():\n",
    "    dataset_folder.mkdir(parents=True)\n",
    "\n",
    "dataset_zip_path = dataset_folder.joinpath(\"dependency_treebank.zip\")\n",
    "if not dataset_zip_path.exists():\n",
    "    print(\"Downloading dataset... \", end=\"\")\n",
    "    download_url(url=dataset_url, download_path=dataset_zip_path)\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(\"Dataset already downloaded!\")\n",
    "dataset_path = dataset_folder.joinpath(dataset_name)\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    print(\"Extracting dataset... (it may take a while...) \", end=\"\")\n",
    "    shutil.unpack_archive(dataset_zip_path, dataset_folder)\n",
    "    print(\"Extraction completed!\")\n",
    "else:\n",
    "    print(\"Dataset already extracted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the corpus into a pandas DataFrame object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = dataset_folder.joinpath(dataset_name)\n",
    "\n",
    "\n",
    "dataframe_rows = []\n",
    "for file_path in sorted(folder.glob(\"*.dp\")):\n",
    "    with file_path.open(mode=\"r\", encoding=\"utf-8\") as text_file:\n",
    "        # Reading the text\n",
    "        text = text_file.read()\n",
    "        # Split sentences (\\n\\n is used for most NLP datasets to split sentences)\n",
    "        sentences = text.split(\"\\n\\n\")\n",
    "\n",
    "        # Observing each sentence\n",
    "        for s in sentences:\n",
    "            sentence = []\n",
    "            tags = []\n",
    "            # sentence = [pierre,vinken,,aksjdajs, ]. tags = [NNP,aab,asd....]\n",
    "            # Taking every line\n",
    "            for line in s.split(\"\\n\"):\n",
    "                columns = line.split(\"\\t\")\n",
    "                # If every line have word, tag, value\n",
    "                if len(columns) > 2:\n",
    "                    # Put words and tags into lists\n",
    "                    sentence.append(columns[0])\n",
    "                    tags.append(columns[1])\n",
    "\n",
    "            # Get the File_ID\n",
    "            file_id = int(file_path.stem.split(\"_\")[1])\n",
    "            dataframe_row = {\"file_id\": file_id, \"sentence\": sentence, \"tag\": tags}\n",
    "            dataframe_rows.append(dataframe_row)\n",
    "# Create the dataframe\n",
    "df = pd.DataFrame(dataframe_rows)\n",
    "\n",
    "FILE_ID, SENTENCE, TAGS = df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[Pierre, Vinken, ,, 61, years, old, ,, will, j...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Mr., Vinken, is, chairman, of, Elsevier, N.V....</td>\n",
       "      <td>[NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[Rudolph, Agnew, ,, 55, years, old, and, forme...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[A, form, of, asbestos, once, used, to, make, ...</td>\n",
       "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>[The, asbestos, fiber, ,, crocidolite, ,, is, ...</td>\n",
       "      <td>[DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_id                                           sentence  \\\n",
       "0        1  [Pierre, Vinken, ,, 61, years, old, ,, will, j...   \n",
       "1        1  [Mr., Vinken, is, chairman, of, Elsevier, N.V....   \n",
       "2        2  [Rudolph, Agnew, ,, 55, years, old, and, forme...   \n",
       "3        3  [A, form, of, asbestos, once, used, to, make, ...   \n",
       "4        3  [The, asbestos, fiber, ,, crocidolite, ,, is, ...   \n",
       "\n",
       "                                                 tag  \n",
       "0  [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...  \n",
       "1  [NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...  \n",
       "2  [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...  \n",
       "3  [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...  \n",
       "4  [DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Data Train-Test-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### file indices for train/validation/test\n",
    "train_ids = np.arange(1, 101)\n",
    "val_ids = np.arange(101, 151)\n",
    "test_ids = np.arange(151, 200)\n",
    "\n",
    "df_train = df[df[FILE_ID].isin(train_ids)]\n",
    "df_val = df[df[FILE_ID].isin(val_ids)]\n",
    "df_test = df[df[FILE_ID].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 2 - 0.5 points] Text encoding\n",
    "\n",
    "To train a neural POS tagger, you first need to encode text into numerical format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Embed words using **GloVe embeddings**.\n",
    "* You are **free** to pick any embedding dimension.\n",
    "* [Optional] You are free to experiment with text pre-processing: **make sure you do not delete any token!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### download Glove\n",
    "\n",
    "embedding_dim = 100\n",
    "glove_file = f\"glove.6B.{embedding_dim}d.txt\"\n",
    "glove_zip = \"glove.6B.zip\"\n",
    "glove_path = Path.cwd().joinpath(glove_file)\n",
    "if not glove_path.exists():\n",
    "    urllib.request.urlretrieve(\n",
    "        \"http://nlp.stanford.edu/data/glove.6B.zip\", Path.cwd().joinpath(glove_zip)\n",
    "    )\n",
    "    with zipfile.ZipFile(glove_zip, \"r\") as zip_ref:\n",
    "        zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_sequence_length = int(np.quantile([len(seq) for seq in df_train[\"sentence\"]], 0.99))\n",
    "max_sequence_length = max([len(seq) for seq in df_train[SENTENCE]])\n",
    "\n",
    "# TODO remove\n",
    "hparams = {\n",
    "    \"batch_size\": 128,\n",
    "    \"embedding_dim\": 100,\n",
    "    \"embedding_trainable\": False,\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"max_sequence_length\": max_sequence_length,\n",
    "    \"vocab_size\": 7405,\n",
    "    \"tag_size\": 46,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing: lowercase\n",
    "def make_lower(df):\n",
    "    df[SENTENCE].apply(lambda l: [i.lower() for i in l])\n",
    "\n",
    "\n",
    "make_lower(df_train)\n",
    "make_lower(df_val)\n",
    "# TODO va bene fare lowercase sul test set? vediamo se va senza\n",
    "# make_lower(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use Keras Tokenizer to create Vocabulary\n",
    "\n",
    "# TODO ci hanno consigliato codice deprecato (tokenizer) io vorrei usare TextVectorization ma fa schifo\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"OOV\")\n",
    "tokenizer.fit_on_texts(df_train[\"sentence\"])\n",
    "\n",
    "tag_tokenizer = Tokenizer()\n",
    "tag_tokenizer.fit_on_texts(df_train[\"tag\"])\n",
    "\n",
    "\n",
    "### Turn text into into padded sequences.\n",
    "def prep_text(texts, tokenizer, max_sequence_length):\n",
    "    text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return sequence.pad_sequences(\n",
    "        text_sequences, maxlen=max_sequence_length, padding=\"post\"\n",
    "    )\n",
    "\n",
    "\n",
    "text_train = prep_text(df_train[\"sentence\"], tokenizer, max_sequence_length)\n",
    "text_test = prep_text(df_test[\"sentence\"], tokenizer, max_sequence_length)\n",
    "text_val = prep_text(df_val[\"sentence\"], tokenizer, max_sequence_length)\n",
    "\n",
    "tag_train = prep_text(df_train[\"tag\"], tag_tokenizer, max_sequence_length)\n",
    "tag_test = prep_text(df_test[\"tag\"], tag_tokenizer, max_sequence_length)\n",
    "tag_val = prep_text(df_val[\"tag\"], tag_tokenizer, max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  2,  7, 11,  5,  6,  7, 20, 12,  4,  1,  3,  4,  6,  1,  2, 11,\n",
       "        8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_train.shape\n",
    "tag_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encode the tags in 1hot encoding\n",
    "\n",
    "num_classes = len(tag_tokenizer.word_index) + 1\n",
    "tag_categorical_train = to_categorical(tag_train, num_classes)\n",
    "tag_categorical_test = to_categorical(tag_test, num_classes)\n",
    "tag_categorical_val = to_categorical(tag_val, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags: ['nn', 'nnp', 'in', 'dt', 'nns', 'jj', ',', '.', 'vbd', 'rb', 'cd', 'vb', 'cc', 'vbz', 'vbn', 'to', 'prp', 'vbg', 'vbp', 'md', 'prp$', '``', 'pos', \"''\", '$', ':', 'wdt', 'jjr', 'wp', 'rp', 'nnps', 'jjs', 'wrb', 'rbr', '-rrb-', '-lrb-', 'ex', 'rbs', 'ls', 'pdt', 'wp$', 'fw', 'uh', 'sym', '#']\n",
      "All tag-tokens: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]\n",
      "\n",
      "Punctuations: [',', '.', ':', '``', \"''\", '$', '#', 'sym', '-rrb-', '-lrb-']\n",
      "Tokenized punctuations [7, 8, 26, 22, 24, 25, 45, 44, 35, 36]\n",
      "\n",
      "Tags without punctuation: ['nn', 'nnp', 'in', 'dt', 'nns', 'jj', 'vbd', 'rb', 'cd', 'vb', 'cc', 'vbz', 'vbn', 'to', 'prp', 'vbg', 'vbp', 'md', 'prp$', 'pos', 'wdt', 'jjr', 'wp', 'rp', 'nnps', 'jjs', 'wrb', 'rbr', 'ex', 'rbs', 'ls', 'pdt', 'wp$', 'fw', 'uh']\n",
      "Tokens will be used in evaluations: [1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 27, 28, 29, 30, 31, 32, 33, 34, 37, 38, 39, 40, 41, 42, 43]\n"
     ]
    }
   ],
   "source": [
    "all_classes = list(tag_tokenizer.word_index.keys())\n",
    "all_tokens = list(tag_tokenizer.word_index.values())\n",
    "punct_classes = [\",\", \".\", \":\", \"``\", \"''\", \"$\", \"#\", \"sym\", \"-rrb-\", \"-lrb-\"]\n",
    "punct_tokens = [tag_tokenizer.word_index[p] for p in punct_classes]\n",
    "allowed_classes = [\n",
    "    word for word in tag_tokenizer.index_word.values() if word not in punct_classes\n",
    "]\n",
    "allowed_tokens = [token for token in all_tokens if token not in punct_tokens]\n",
    "\n",
    "print(f\"Tags: {all_classes}\")\n",
    "print(f\"All tag-tokens: {all_tokens}\\n\")\n",
    "print(f\"Punctuations: {punct_classes}\")\n",
    "print(f\"Tokenized punctuations {punct_tokens}\\n\")\n",
    "print(f\"Tags without punctuation: {allowed_classes}\")\n",
    "print(f\"Tokens will be used in evaluations: {allowed_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "### Create the embedding matrix\n",
    "embeddings_index = {}\n",
    "with open(glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(f\"Found {len(embeddings_index)} word vectors.\")\n",
    "\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "num_words_in_embedding = 0\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        num_words_in_embedding += 1\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: the \n",
      "Vector: [-0.10767     0.11053     0.59811997 -0.54360998  0.67395997  0.10663\n",
      "  0.038867    0.35481     0.06351    -0.094189    0.15786    -0.81664997\n",
      "  0.14172     0.21939     0.58504999 -0.52157998  0.22782999 -0.16642\n",
      " -0.68228     0.35870001  0.42568001  0.19021     0.91962999  0.57555002\n",
      "  0.46184999  0.42363    -0.095399   -0.42749    -0.16566999 -0.056842\n",
      " -0.29595     0.26036999 -0.26605999 -0.070404   -0.27662     0.15820999\n",
      "  0.69825     0.43081     0.27952    -0.45436999 -0.33801001 -0.58183998\n",
      "  0.22363999 -0.57779998 -0.26862001 -0.20424999  0.56393999 -0.58524001\n",
      " -0.14365    -0.64218003  0.0054697  -0.35247999  0.16162001  1.1796\n",
      " -0.47674    -2.75530005 -0.1321     -0.047729    1.06550002  1.10339999\n",
      " -0.2208      0.18669     0.13177     0.15117     0.71310002 -0.35214999\n",
      "  0.91347998  0.61782998  0.70991999  0.23954999 -0.14571001 -0.37858999\n",
      " -0.045959   -0.47367999  0.2385      0.20536    -0.18996     0.32506999\n",
      " -1.11119998 -0.36341     0.98679    -0.084776   -0.54008001  0.11726\n",
      " -1.0194     -0.24424     0.12771     0.013884    0.080374   -0.35414001\n",
      "  0.34951001 -0.72259998  0.37549001  0.44409999 -0.99058998  0.61214\n",
      " -0.35111001 -0.83155     0.45293     0.082577  ]\n"
     ]
    }
   ],
   "source": [
    "### Inspect tokens' embedding vectors\n",
    "idx_token = 2\n",
    "print(\n",
    "    f\"Token: {list(tokenizer.word_index.keys())[idx_token]} \\nVector: {embedding_matrix[idx_token]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fare un po' di inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_layer = Embedding(\n",
    "#    num_tokens,\n",
    "#    embedding_dim,\n",
    "#    trainable=False,\n",
    "# )\n",
    "# embedding_layer.build((1,))\n",
    "# embedding_layer.set_weights([embedding_matrix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 3 - 1.0 points] Model definition\n",
    "\n",
    "You are now tasked to define your neural POS tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* **Baseline**: implement a Bidirectional LSTM with a Dense layer on top.\n",
    "* You are **free** to experiment with hyper-parameters to define the baseline model.\n",
    "\n",
    "* **Model 1**: add an additional LSTM layer to the Baseline model.\n",
    "* **Model 2**: add an additional Dense layer to the Baseline model.\n",
    "\n",
    "* **Do not mix Model 1 and Model 2**. Each model has its own instructions.\n",
    "\n",
    "**Note**: if a document contains many tokens, you are **free** to split them into chunks or sentences to define your mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(lstm_units=64, add_lstm=False, add_dense=False):\n",
    "    number_of_words = len(tokenizer.word_index.keys())\n",
    "    number_of_tags = len(tag_tokenizer.word_index.keys())\n",
    "\n",
    "    input = Input(shape=(max_sequence_length,))\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=number_of_words + 1,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=max_sequence_length,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=False,\n",
    "    )(input)\n",
    "    bi_lstm = Bidirectional(LSTM(lstm_units, return_sequences=True))(embedding_layer)\n",
    "    bi_lstm2 = (\n",
    "        Bidirectional(LSTM(lstm_units, return_sequences=True))(bi_lstm)\n",
    "        if add_lstm\n",
    "        else bi_lstm\n",
    "    )\n",
    "    dense2 = (\n",
    "        TimeDistributed(Dense(number_of_tags + 1, activation=\"relu\"))(bi_lstm2)\n",
    "        if add_dense\n",
    "        else bi_lstm\n",
    "    )\n",
    "    dense_output = TimeDistributed(Dense(number_of_tags + 1, activation=\"softmax\"))(\n",
    "        dense2\n",
    "    )\n",
    "    model = Model(input, dense_output)\n",
    "\n",
    "    ### No logits: we have 1 hot encoding as labels\n",
    "    model.compile(loss=CategoricalCrossentropy(from_logits=False), optimizer=Adam(1e-3))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"f1_score\", mode=\"max\", patience=5, restore_best_weights=True\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"f1_score\", mode=\"max\", factor=0.1, patience=3, min_lr=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 4 - 1.0 points] Metrics\n",
    "\n",
    "Before training the models, you are tasked to define the evaluation metrics for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Evaluate your models using macro F1-score, compute over **all** tokens.\n",
    "* **Concatenate** all tokens in a data split to compute the F1-score. (**Hint**: accumulate FP, TP, FN, TN iteratively) \n",
    "* **Do not consider punctuation and symbol classes** $\\rightarrow$ [What is punctuation?](https://en.wikipedia.org/wiki/English_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Note**: What about OOV tokens?\n",
    "   * All the tokens in the **training** set that are not in GloVe are **not** considered as OOV\n",
    "   * For the remaining tokens (i.e., OOV in the validation and test sets), you have to assign them a **static** embedding.\n",
    "   * You are **free** to define the static embedding using any strategy (e.g., random, neighbourhood, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(model):\n",
    "    # TODO what is a static embedding?\n",
    "\n",
    "    # TODO controllare che questo vada bene: fare la prova accumulando manualmente TP FP TN FN\n",
    "\n",
    "    y_pred = model.predict([text_val]).argmax(-1).flatten()\n",
    "    y_test_flatten = tag_categorical_val.argmax(-1).flatten()\n",
    "    score = f1_score(\n",
    "        y_test_flatten,\n",
    "        y_pred,\n",
    "        labels=allowed_tokens,\n",
    "        average=\"macro\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 5 - 1.0 points] Training and Evaluation\n",
    "\n",
    "You are now tasked to train and evaluate the Baseline, Model 1, and Model 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Train **all** models on the train set.\n",
    "* Evaluate **all** models on the validation set.\n",
    "* Compute metrics on the validation set.\n",
    "* Pick **at least** three seeds for robust estimation.\n",
    "* Pick the **best** performing model according to the observed validation set performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(seed, model1=False, model2=False):\n",
    "    set_reproducibility(seed)\n",
    "\n",
    "    model = get_model(add_lstm=model1, add_dense=model2)\n",
    "\n",
    "    ### train model on the train set\n",
    "    history = model.fit(\n",
    "        text_train,\n",
    "        tag_categorical_train,\n",
    "        batch_size=64,\n",
    "        epochs=50,\n",
    "        validation_data=(text_val, tag_categorical_val),\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "    )\n",
    "\n",
    "    ### compute metrics\n",
    "    score = metric(model)\n",
    "\n",
    "    return score, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [333, 666, 999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 249)]             0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 249, 100)          740600    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 249, 128)          84480     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 249, 45)           5805      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 830885 (3.17 MB)\n",
      "Trainable params: 90285 (352.68 KB)\n",
      "Non-trainable params: 740600 (2.83 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/leonardo/Projects/NLP_aSSIGNMENT_1/NLP/.venv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/leonardo/Projects/NLP_aSSIGNMENT_1/NLP/.venv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/leonardo/Projects/NLP_aSSIGNMENT_1/NLP/.venv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/leonardo/Projects/NLP_aSSIGNMENT_1/NLP/.venv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/leonardo/Projects/NLP_aSSIGNMENT_1/NLP/.venv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"/home/leonardo/Projects/NLP_aSSIGNMENT_1/NLP/.venv/lib/python3.8/site-packages/keras/src/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/leonardo/Projects/NLP_aSSIGNMENT_1/NLP/.venv/lib/python3.8/site-packages/keras/src/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/leonardo/Projects/NLP_aSSIGNMENT_1/NLP/.venv/lib/python3.8/site-packages/keras/src/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/leonardo/Projects/NLP_aSSIGNMENT_1/NLP/.venv/lib/python3.8/site-packages/keras/src/losses.py\", line 2122, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/leonardo/Projects/NLP_aSSIGNMENT_1/NLP/.venv/lib/python3.8/site-packages/keras/src/backend.py\", line 5560, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 249, 46) and (None, 249, 45) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### baseline\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m baseline_results \u001b[38;5;241m=\u001b[39m [experiment(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m seeds]\n\u001b[1;32m      3\u001b[0m baseline_scores \u001b[38;5;241m=\u001b[39m [r[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m baseline_results]\n\u001b[1;32m      4\u001b[0m baseline_histories \u001b[38;5;241m=\u001b[39m [r[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m baseline_results]\n",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### baseline\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m baseline_results \u001b[38;5;241m=\u001b[39m [\u001b[43mexperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m seeds]\n\u001b[1;32m      3\u001b[0m baseline_scores \u001b[38;5;241m=\u001b[39m [r[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m baseline_results]\n\u001b[1;32m      4\u001b[0m baseline_histories \u001b[38;5;241m=\u001b[39m [r[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m baseline_results]\n",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m, in \u001b[0;36mexperiment\u001b[0;34m(seed, model1, model2)\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(add_lstm\u001b[38;5;241m=\u001b[39mmodel1, add_dense\u001b[38;5;241m=\u001b[39mmodel2)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m### train model on the train set\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtag_categorical_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag_categorical_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m### compute metrics\u001b[39;00m\n\u001b[1;32m     17\u001b[0m score \u001b[38;5;241m=\u001b[39m metric(model)\n",
      "File \u001b[0;32m~/Projects/NLP_aSSIGNMENT_1/NLP/.venv/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filehcgiaeof.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/leonardo/Projects/NLP_aSSIGNMENT_1/NLP/.venv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/leonardo/Projects/NLP_aSSIGNMENT_1/NLP/.venv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/leonardo/Projects/NLP_aSSIGNMENT_1/NLP/.venv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/leonardo/Projects/NLP_aSSIGNMENT_1/NLP/.venv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/leonardo/Projects/NLP_aSSIGNMENT_1/NLP/.venv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"/home/leonardo/Projects/NLP_aSSIGNMENT_1/NLP/.venv/lib/python3.8/site-packages/keras/src/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/leonardo/Projects/NLP_aSSIGNMENT_1/NLP/.venv/lib/python3.8/site-packages/keras/src/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/leonardo/Projects/NLP_aSSIGNMENT_1/NLP/.venv/lib/python3.8/site-packages/keras/src/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/leonardo/Projects/NLP_aSSIGNMENT_1/NLP/.venv/lib/python3.8/site-packages/keras/src/losses.py\", line 2122, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/leonardo/Projects/NLP_aSSIGNMENT_1/NLP/.venv/lib/python3.8/site-packages/keras/src/backend.py\", line 5560, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 249, 46) and (None, 249, 45) are incompatible\n"
     ]
    }
   ],
   "source": [
    "### baseline\n",
    "baseline_results = [experiment(s) for s in seeds]\n",
    "baseline_scores = [r[0] for r in baseline_results]\n",
    "baseline_histories = [r[1] for r in baseline_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[plt.plot(h) for h in baseline_histories]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model1\n",
    "model1_results = [experiment(s, model1=True) for s in seeds]\n",
    "model1_scores = [r[0] for r in model1_results]\n",
    "model1_histories = [r[1] for r in model1_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[plt.plot(h) for h in model1_histories]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model2\n",
    "model2_results = [experiment(s, model2=True) for s in seeds]\n",
    "model2_scores = [r[0] for r in model2_results]\n",
    "model2_histories = [r[1] for r in model2_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[plt.plot(h) for h in model2_histories]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 6 - 1.0 points] Error Analysis\n",
    "\n",
    "You are tasked to evaluate your best performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Compare the errors made on the validation and test sets.\n",
    "* Aggregate model errors into categories (if possible) \n",
    "* Comment the about errors and propose possible solutions on how to address them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Task 7 - 1.0 points] Report\n",
    "\n",
    "Wrap up your experiment in a short report (up to 2 pages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Use the NLP course report template.\n",
    "* Summarize each task in the report following the provided template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recommendations\n",
    "\n",
    "The report is not a copy-paste of graphs, tables, and command outputs.\n",
    "\n",
    "* Summarize classification performance in Table format.\n",
    "* **Do not** report command outputs or screenshots.\n",
    "* Report learning curves in Figure format.\n",
    "* The error analysis section should summarize your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Submission\n",
    "\n",
    "* **Submit** your report in PDF format.\n",
    "* **Submit** your python notebook.\n",
    "* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc...\n",
    "* You can upload **model weights** in a cloud repository and report the link in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# FAQ\n",
    "\n",
    "Please check this frequently asked questions before contacting us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Trainable Embeddings\n",
    "\n",
    "You are **free** to define a trainable or non-trainable Embedding layer to load the GloVe embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model architecture\n",
    "\n",
    "You **should not** change the architecture of a model (i.e., its layers).\n",
    "\n",
    "However, you are **free** to play with their hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Neural Libraries\n",
    "\n",
    "You are **free** to use any library of your choice to implement the networks (e.g., Keras, Tensorflow, PyTorch, JAX, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Keras TimeDistributed Dense layer\n",
    "\n",
    "If you are using Keras, we recommend wrapping the final Dense layer with `TimeDistributed`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Error Analysis\n",
    "\n",
    "Some topics for discussion include:\n",
    "   * Model performance on most/less frequent classes.\n",
    "   * Precision/Recall curves.\n",
    "   * Confusion matrices.\n",
    "   * Specific misclassified samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Punctuation\n",
    "\n",
    "**Do not** remove punctuation from documents since it may be helpful to the model.\n",
    "\n",
    "You should **ignore** it during metrics computation.\n",
    "\n",
    "If you are curious, you can run additional experiments to verify the impact of removing punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
